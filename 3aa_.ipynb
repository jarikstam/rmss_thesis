{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enabling-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize\n",
    "import pickle\n",
    "import regex as re\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "african-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the submissions to manually annotate them\n",
    "submissions_file = './data/submissions.csv'\n",
    "annotated_submissions_file = './data/submissions_annotated.csv'\n",
    "\n",
    "# Comments from selected submissions\n",
    "comments_file = './data/comments.csv'\n",
    "\n",
    "# Tokenized sentences for processing with word2vec\n",
    "tokens_file = './data/tokens.txt'\n",
    "tokenized_sents_file = 'data/tokenized_sents.txt'\n",
    "ngrams_file = './data/ngrams'\n",
    "\n",
    "# Tokenized comments for matching discourse atoms and concept mover's distance to comments\n",
    "tokenized_comments_file = './data/tokenized_comments.p' #del\n",
    "tokenized_comments_v2_file = './data/tokenized_comments' #del? or needed for cmd?\n",
    "tokenized_comments_text_file = './data/tokenized_comments.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-product",
   "metadata": {},
   "source": [
    "## Tokenize comments\n",
    "- Convert comments to lowercase, replace accented letters\n",
    "- Split comments into sentences and make sure all sentences are unique\n",
    "- Then split sentences into tokens using TreebankWordTokenizer, removing tokens that are only punctuation\n",
    "- Finally save the sentences (one sentence per line) which is the input required for Gensim word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "macro-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(comments_file, sep=';')['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complete-reasoning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ADDITIONALLY-- thank you so much for caring enough to ask, in a genuinely curious and respectful way. I'm curious to hear your thoughts!\",\n",
       " 'I am watching Shape of Water.  It says some stuff in Russian.  How can I find out what they are saying?',\n",
       " 'Extremely underwhelmed by the film.\\n\\nIt just felt dull, the \\'romance\\' between fish-dick and mute-chick was too quick and I never felt any connection or empathy for the \\'asset\\'. \\n\\nThe pie-shop scenes were so quick and short, and ultimately meant nothing. \"I want to get to know you\" , \"Ew, no.\" and that was it. Just the film saying \"Hah, look! The 40\\'s hated gays!\"\\n\\nThe strange musical imagination scene was a complete tonal change from the rest of the film. \\n\\nAnd it seems the film just ignored all logic to tell the story. The \\'asset\\' had no security at all, no guards, no cameras. The cleaner was allowed in and out whenever she felt like it most of the time. When the guy\\'s hand was injured, it took ages for any help to arrive - in a top-security lab?\\n\\nA cleaner and her elderly friend manage to break out the creature in a top-security lab as well, and there was only ONE guard in the entire place, apparently. They pretty much just walk it out the back door, and no one gives chase. It just felt completely off.\\n\\nAnd I\\'ll ignore the bathroom that was strong enough to fill up with water, which completely disregarded physics.\\n\\nThe film was utterly inconsistent, and did nothing memorable or new.',\n",
       " '\"We need a quirky \\'mute\\' girl. We need a gay guy. We need a black sidekick, we need a fully evil white guy in a suit, we need the Russians to try to fuck everything up.\"\\n\\nIt does seem to tick all the boxes.\\n\\nI also hated the movie, but not for those reasons. It just felt dull, the \\'romance\\' between fish-dick and mute-chick was too quick and I never felt any connection or empathy for the \\'asset\\'. \\n\\nThe strange musical imagination scene was a complete tonal change from the rest of the film. \\n\\nAnd it seems the film just ignored all logic to tell the story. The \\'asset\\' had no security at all, no guards, no cameras. The cleaner was allowed in and out whenever she felt like it most of the time. When the guy\\'s hand was injured, it took ages for any help to arrive - in a top-security lab?\\n\\nA cleaner and her elderly friend manage to break out the creature in a top-security lab as well, and there was only ONE guard in the entire place, apparently. They pretty much just walk it out the back door. It just felt completely off.\\n\\nAnd I\\'ll ignore the bathroom that was strong enough to fill up with water, which completely disregarded physics.\\n\\nThe film was completely inconsistent, and did nothing memorable or new.',\n",
       " 'But she wasn\\'t either.  She was part fish and had no idea.  Her scars were actually gills.  She dreamed of water.  She was found as a baby near the water alone.  This leads me to believe that she was some sort of hybrid between man and the \"asset\\'s\" species.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentences = set()\n",
    "\n",
    "for comment in tqdm(comments):\n",
    "    comment = str(comment)\n",
    "    comment = comment.lower()\n",
    "    comment = utils.strip_accents(comment)\n",
    "    \n",
    "    for sent in sent_tokenize(comment):\n",
    "        sent = sent.strip()\n",
    "        sent = utils.tokenize_sentence(sent)\n",
    "        text_sent = \" \".join(sent)\n",
    "        unique_sentences.add(text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "print(len(unique_sentences))\n",
    "\n",
    "for i in unique_sentences:\n",
    "    if x == 5:\n",
    "        break\n",
    "    print(i)\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tokens_file, 'w') as f:\n",
    "    for sent in tqdm(unique_sentences):\n",
    "        f.write(sent+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "charged-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn ngrams\n",
    "sentences = LineSentence(tokens_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rational-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = Phrases(sentences, connector_words=ENGLISH_CONNECTOR_WORDS, min_count=15)\n",
    "ngrams.save(ngrams_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confirmed-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = Phrases.load(ngrams_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "confused-subscriber",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5400518it [02:12, 40863.92it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(tokenized_sents_file, 'w') as f:\n",
    "    for sent in tqdm(sentences):\n",
    "        tokenized_sent = ngrams[sent]\n",
    "        text_sent = \" \".join(tokenized_sent)\n",
    "        f.write(text_sent+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "green-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'whole', 'catacombs', 'scene', 'was', 'amazing']\n",
      "['maybe', 'youre', 'on', 'to', 'something', 'about', 'it', 'being', 'experimental', 'and', 'i', 'think', 'he', 'experimented', 'either', 'too_much', 'or', 'not', 'enough', 'with', 'this', 'one']\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "for i in sentences:\n",
    "    print(ngrams[i])\n",
    "    x+=1\n",
    "    if x == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "structural-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2116273/2116273 [12:29<00:00, 2822.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# del?\n",
    "with open(tokenized_comments_v2_file, 'wb') as f:\n",
    "    \n",
    "    for comment in tqdm(comments):\n",
    "        comment = str(comment)\n",
    "        comment = comment.lower()\n",
    "        comment = utils.strip_accents(comment)\n",
    "\n",
    "        tokenized_comment = []\n",
    "\n",
    "        for sent in sent_tokenize(comment):\n",
    "            sent = sent.strip()\n",
    "            sent = utils.tokenize_sentence(sent)\n",
    "            x = ngrams[sent]\n",
    "            tokenized_comment.extend(x)\n",
    "\n",
    "        pickle.dump(tokenized_comment, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "piano-benefit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2116273/2116273 [12:11<00:00, 2894.82it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(tokenized_comments_text_file, 'w') as f:\n",
    "    \n",
    "    for comment in tqdm(comments):\n",
    "        comment = str(comment)\n",
    "        comment = comment.lower()\n",
    "        comment = utils.strip_accents(comment)\n",
    "\n",
    "        tokenized_comment = \"\"\n",
    "\n",
    "        for sent in sent_tokenize(comment):\n",
    "            sent = sent.strip()\n",
    "            sent = utils.tokenize_sentence(sent)\n",
    "            x = ngrams[sent]\n",
    "            tokenized_comment += \" \".join(x)+\" \"\n",
    "            \n",
    "        tokenized_comment = tokenized_comment.strip()\n",
    "        f.write(tokenized_comment+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
