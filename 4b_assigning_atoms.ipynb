{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: copied from: https://github.com/arsena-k/discourse_atoms/blob/master/DATM%20Tutorial%20Part%202%20of%202%20-%20Assigning%20Atoms%20-%20for%20Public.ipynb\n",
    "\n",
    "My notes (18-02-2022):\n",
    "- I had to update the code for Gensim 4.0, which I think I did correctly (otherwise it probably wouldn't work)\n",
    "- I also didn't quite understand what datatype their corpus was. I thought it was lists of tokens nested inside a list of comments, but it seems it was actually a dataframe. From past experience I think pandas dataframes aren't very good at being iterated over, so I avoided using dataframes as much as possible. Only at the end I create a dataframe, but even then I think it's better to store it as a sparse matrix rather than a csv file.\n",
    "- Also since I'm not combining two datasets as they do, I skipped half the parts they do double. \n",
    "- In the end it seems every went correctly, but tbh I'm in over my head so I am not certain\n",
    "\n",
    "\n",
    "# Discourse Atom Topic Modeling (DATM) Tutorial \n",
    "\n",
    "## Part 2 of 2: Mapping Atoms to Text Data\n",
    "\n",
    "* This code is written in Python 3.7.2, and uses Gensim version 3.8.3. \n",
    "\n",
    "* This code is provides an the outline of how we assigned atoms to our cleaned data, which we show how to identify in Part 1 of 2. Note that we cannot redistribute the data used in our paper \"Integrating Topic Modeling and Word Embedding\" in any form, and researchers must apply directly to the Centers for Disease Control and Prevention for access. Details on data access are provided in the paper. We add comments with tips for adapting this code to your data.  \n",
    "* In our case, the goal of this code is to take a given narrative, get rolling windows of contexts from this narrative, find the SIF sentence embedding from each rolling window, and match the SIF embedding onto the closest (by cosine similarity) atom in the Dictionary loaded in earler. The SIF embedding is the maximum a posteriori (MAP) estimate of what the atom is for that sentence. So we'll get out a rolling window (i.e., a sequence) of atoms underlying the narrative.\n",
    "* In our case, we get atoms separately for law enforcement (narle) and medical examiner (narcme) narratives, and then combine the two distributions, as described in our paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all of these are being used. \n",
    "\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import cython\n",
    "import pickle\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.preprocessing import normalize\n",
    "from random import sample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import math\n",
    "from scipy.linalg import norm\n",
    "from collections import Counter\n",
    "from ksvd import ApproximateKSVD \n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "import string, re\n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from random import seed, sample\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in a word embedding model trained on the corpus (in our case, violent death narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel=Word2Vec.load('./models/gensim_model_window10_vector_300')\n",
    "#w2vmodel.init_sims(replace=False) #normalize word-vector lengths. May speed up if set replace=True, but then can't go back to the original (non-normalized) word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in dictionary of atoms identified in your word embedding (see \"DATM Tutorial Part 1 of 2\" for code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load back in the pickled dictionary of atom vectors\n",
    "infile = open('./datm/250comp5nonzeros_dictionary','rb')\n",
    "mydictionary=pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare two pieces of informatin from the text data, which we will need to compute SIF Sentence Embeddings (MAP) of any given sentence \n",
    "\n",
    "* SIF Sentence Embedding is from: \"A Simple but tough-to-beat baseline for sentence embedding\" https://github.com/PrincetonML/SIF\n",
    "* To do SIF embeddings, we need to prep functions and two pieces of information from the raw text: (1) frequency weights for each word, and (2) the \"common discourse vector\" ($C_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ngram_comments.p', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "#this is the text data which has already been turned into trigrams and bigrams, cleaned, and tokenized. It is a list of lists, where each record is a word tokenized \"document\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The first input to SIF embeddings is an estimate of the frequency weights (based on probabilites) for each word in the corpus. Compute this here.**\n",
    "* This will naturally downweight stopwords when we compute a sentence embedding. It requires the raw text data of the corpus. \n",
    "* Either train a dictonary of weights (1), or upload a saved dictionary (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_dict(w2vmodel, weight_a=.001): #reasonable range for weight a is .001 to .0001 based on Arora et al SIF embeddings. The extent to which we re-weight words is controlled by the parameter $a$, where a lower value for $a$ means that frequent words are more aggressively down-weighted compared to less frequent words. \n",
    "    freq_dictionary = {word: w2vmodel.wv.get_vecattr(word, \"count\") for word in w2vmodel.wv.index_to_key} \n",
    "    total= sum(freq_dictionary.values())\n",
    "    freq_dictionary = {word: weight_a/(weight_a + (w2vmodel.wv.get_vecattr(word, \"count\") / total)) for word in w2vmodel.wv.index_to_key} #best values according to arora et al are between .001 and .0001\n",
    "    return(freq_dictionary)\n",
    "\n",
    "#function to yield a weighted sentence, using the above weight dictionary\n",
    "def get_weighted_sent(tokedsent,freq_dict, w2vmodel=w2vmodel): \n",
    "    weightedsent= [freq_dict[word]*w2vmodel.wv[word] for word in tokedsent if word in freq_dict.keys()]\n",
    "    return(sum(weightedsent)/len(weightedsent)) #weightedsent_avg  #divide the weighted average by the number of words in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict= get_freq_dict(w2vmodel, weight_a=.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The second input to SIF embeddings is $C_0$, the common discourse vector. Compute this here.**\n",
    "* Get this with a random sample of discourse vectors since the data is so large, or compute using all narratives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samp_cts(docs, n_sample, windowsize, freq_dictionary):\n",
    "    sampnarrs=  sample(docs, n_sample) #sample of narratives. Will take 1 random window and discourse vector of this window, from each narrative. \n",
    "    sampvecs= []\n",
    "\n",
    "\n",
    "    t1_start = perf_counter()  \n",
    "\n",
    "    for i in sampnarrs: #adjusting here to corpus sample, but consider using full corpus for final SIF embeddings. \n",
    "        if len(i)>windowsize: #want window length to be at least windowsize words\n",
    "            n= sample(range(0,len(i)-windowsize), 1)[0] #get some random positon in the narrative (at least windowsize steps behind the last one though)\n",
    "            sent= i[n:n+windowsize] #random context window \n",
    "            sampvecs.append(get_weighted_sent(i, freq_dictionary)) #sample a discourse vector, and append to a list of sample discourse vectors.\n",
    "            n= sample(range(0,len(i)-windowsize), 1)[0] #get some random positon in the narrative (at least windowsize steps behind the last one though)\n",
    "            sent= i[n:n+windowsize] #random context window \n",
    "            sampvecs.append(get_weighted_sent(i, freq_dictionary)) #sample a discourse vector, and append to a list of sample discourse vectors.\n",
    "    sampvecs= np.asarray(sampvecs)\n",
    "    t1_stop = perf_counter() #for 100k context windows takes  \n",
    "    print(t1_stop-t1_start)\n",
    "    return(sampvecs)\n",
    "\n",
    "def get_c0(sampvecs):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=10, random_state=0) #only keeping top component, using same method as in SIF embedding code\n",
    "    svd.fit(sampvecs) #1st singular vector  is now c_o\n",
    "    return(svd.components_[0])\n",
    "\n",
    "def remove_c0(comdiscvec, modcontextvecs):\n",
    "    curcontextvec= [X - X.dot(comdiscvec.transpose()) * comdiscvec for X in modcontextvecs] #remove c_0 from all the cts\n",
    "    curcontextvec=np.asarray(modcontextvecs)\n",
    "    return(curcontextvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.106024299999262\n"
     ]
    }
   ],
   "source": [
    "# Why a sample, why 50,000? Should I do 100,000 since I only do one corpus?\n",
    "# Also: I could replace 'sample(corpus, 50000)' with 'corpus', since the sampling happens inside the function.\n",
    "\n",
    "seed(5)\n",
    "sampvecs2_narcme= samp_cts(sample(corpus, 50000), 50000, 10, freq_dict) #we used a random sample of 50,000 context vectors\n",
    "sampvecs2_narcme= normalize(sampvecs2_narcme, axis=1) #l2 normalize the resulting context vectors\n",
    "\n",
    "#seed(6)\n",
    "\n",
    "#sampvecs2_narle= samp_cts(sample(corpus, 50000), 50000, 10, freq_dict) #we used random sample of 50,000 context vectors\n",
    "#sampvecs2_narle= normalize(sampvecs2_narle, axis=1) #l2 normalize the resulting context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc0_narcme= get_c0(sampvecs2_narcme)\n",
    "#pc0_narle= get_c0(sampvecs2_narle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampvecs2_narcme = remove_c0(pc0_narcme, sampvecs2_narcme) \n",
    "#sampvecs2_narle = remove_c0(pc0_narle, sampvecs2_narle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Resulting function to get SIF MAPs along rolling windows, for a given narrative. \n",
    "\n",
    "* This the function we use to find rolling windows and assign MAPs to them, for a given narrative.\n",
    "* Note that this is set for our embedding size, which was 200-dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sif_atom_seqs(toked_narrative, window_size, topics_dictionary, c0_vector, freq_dict, w2vmodel): \n",
    "    \n",
    "    toked_narr2 = [i for i in toked_narrative if i in w2vmodel.wv.index_to_key] #remove words not in vocab\n",
    "    if len(toked_narr2)> 19 :  #this is set so that only narratives with at least 19 tokens in the w2v model vocab are considered. \n",
    "        it = iter(toked_narr2) \n",
    "        win = [next(it) for cnt in range(0,window_size)] #first context window\n",
    "        MAPs= normalize(remove_c0( c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,300))) #doing the SIF map here. Hardcoding in the dimensionality of the space to speed this up.\n",
    "        for e in it: # Subsequent windows\n",
    "            win[:-1] = win[1:]\n",
    "            win[-1] = e\n",
    "            MAPs = np.vstack((MAPs, normalize(remove_c0(c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,300)))))  #this will be matrix of MAPs\n",
    "\n",
    "        costri= linear_kernel(MAPs, topics_dictionary) \n",
    "        atomsseq= np.argmax(costri, axis=1) #this is for the index of the closest atom to each of the MAPs\n",
    "        #maxinRow = np.amax(costri, axis=1) #this is for the closest atom's cossim value to each of the maps\n",
    "        return(atomsseq.tolist()) #returns sequence of the closest atoms to the MAPs\n",
    "    else:\n",
    "        return(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample usage of sif_atom_seqs, on a single narrative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Documents in a Corpus into a Sequences of Atoms \n",
    "\n",
    "* This is the **final result** we want from all code above\n",
    "* First, get c0 from narcme narratives, and then get the atom sequence for the narcme narratives\n",
    "* Then, get c0 from narle narratives, and then get the atom sequence for the narle narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get SIF Atom Seqs on NARCME narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2118317/2118317 [3:06:33<00:00, 189.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# This took about 3 hours for 2.1 million comments\n",
    "\n",
    "atom_seq = []\n",
    "for x in tqdm(corpus):\n",
    "    x = sif_atom_seqs(x, 10, mydictionary , pc0_narcme, freq_dict, w2vmodel)\n",
    "    if not(x == None):\n",
    "        x = ' '.join([str(elem) for elem in x])\n",
    "    else:\n",
    "        x = ''\n",
    "    \n",
    "    atom_seq.append(x)\n",
    "\n",
    "#df['narcme_atom_seq']= df['narcme_toked'].apply(lambda x: sif_atom_seqs(x, 10, mydictionary , pc0_narcme, freq_dict, w2vmodel) )\n",
    "\n",
    "#convert the atom seq to a string of atoms, since this format is needed for the vectorizer (and easier to work with later) in a CSV, too\n",
    "#df['narcme_atom_seq'] = df['narcme_atom_seq'].apply(lambda x: ' '.join([str(elem) for elem in x])  if(np.all(pd.notnull(x)))  else x ) #https://thispointer.com/python-how-to-convert-a-list-to-string/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'108 108 108 108 95 95 95 95 95 41 41 95 96 96 96 96 96 96 96 96 249 249 88 88 249 88 88 88 88 88 88 172 172 172 172 172 172 172 202 202 202 143 238 162 162 162 238 244 244 244 244 209 209 244 244 244 244 209 209 188 188 188 188 188 188 188 188 188 188 188 188 200 209 209 209 209 185 209 185 185 185 209 185 70 185 185 70 70 247 247 156 156 156 156 156 156 156 156 236 236 236 236 236 236 236 236 54 54 54 54 54 54 41 41 41 41 42 42 42 34 70 4 108 24 24 24 24 24 24 24 8 8 8 64 64 64 64 249 249 249 249 249 249 249 249 249 9 9 249 249 249 249 249 249 249 249 249 249 249 249 249 149 3 249 249 249 249 187 187 187 187 249 249 201 201 201 201 164 164 164 164 164 143 164 164 164 164 201 201 201 201 201 201 201 249 249 249 209 209 209 185 185 244 244'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_seq[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting the Resulting Atom Sequences into Variables, by Vectorizing the Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform each sequence into a distribution over topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = TfidfVectorizer(analyzer = 'word', norm='l1', use_idf=False, token_pattern='\\S+') #need token pattern, otherwise splits using using 0-9 single digits too! #note that atoms that are part of all or no documents will not be transformed here, can reset this default, but I left as is for now since makes prediction easier (fewer features). #includes l1 normalization so that longer documents don't get more weight, l1 normalizes with abs value but all our values are pos anyways\n",
    "#bow_transformer.fit([x for x in atom_seq if x != '']) #corpus needs to be in format ['word word word'], NOT tokenized already\n",
    "bow_transformer.fit(atom_seq)\n",
    "\n",
    "vecked = bow_transformer.transform(atom_seq).toarray() #consider instead:  vecked = bow_transformer.transform(corpus['narcme_narle_atom_seq_combined'].dropna(inplace=True).tolist()).toarray() #this is the \"feature\" data, now in an array for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2118317"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2118317"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vecked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vecked, columns = bow_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([corpus,pd.DataFrame(vecked, columns = bow_transformer.get_feature_names())], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   10  100  101  102  103  104  105  106  ...   90   91   92  \\\n",
       "0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  0.005102  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "    93        94        95        96   97   98   99  \n",
       "0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  \n",
       "1  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  \n",
       "2  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  \n",
       "3  0.0  0.000000  0.029412  0.039216  0.0  0.0  0.0  \n",
       "4  0.0  0.105263  0.000000  0.078947  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"but she was n't either she was part fish and had no_idea her scars were actually gills she dreamed of water she was found as a baby near the water alone this leads me to believe that she was some_sort of hybrid between man and the asset 's species\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(corpus[4])\n",
    "# visual inspection confirms that current topic names (i.e., 1-250 match the discourse atoms)\n",
    "# For example, corpus[4] scores strongly on topic 94, which is she/her and a bunch of female first names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save CSV with final atom assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/atoms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   10  100  101  102  103  104  105  106  ...   90   91   92  \\\n",
       "0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  0.005102  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "    93        94        95        96   97   98   99  \n",
       "0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  \n",
       "1  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  \n",
       "2  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  \n",
       "3  0.0  0.000000  0.029412  0.039216  0.0  0.0  0.0  \n",
       "4  0.0  0.105263  0.000000  0.078947  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save it as a sparse matrix.. waayyy more memory efficient (40 mb versus 2.2gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df.astype(pd.SparseDtype(\"float\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = csr_matrix(df_c.sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2118317x250 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13738155 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(\"./data/atoms.npz\", df_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
