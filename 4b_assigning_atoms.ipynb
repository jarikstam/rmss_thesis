{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: copied from: https://github.com/arsena-k/discourse_atoms/blob/master/DATM%20Tutorial%20Part%202%20of%202%20-%20Assigning%20Atoms%20-%20for%20Public.ipynb\n",
    "\n",
    "My notes (18-02-2022):\n",
    "- I had to update the code for Gensim 4.0, which I think I did correctly (otherwise it probably wouldn't work)\n",
    "- I also didn't quite understand what datatype their corpus was. I thought it was lists of tokens nested inside a list of comments, but it seems it was actually a dataframe. From past experience I think pandas dataframes aren't very good at being iterated over, so I avoided using dataframes as much as possible. Only at the end I create a dataframe, but even then I think it's better to store it as a sparse matrix rather than a csv file.\n",
    "- Also since I'm not combining two datasets as they do, I skipped half the parts they do double. \n",
    "- In the end it seems every went correctly, but tbh I'm in over my head so I am not certain\n",
    "\n",
    "\n",
    "# Discourse Atom Topic Modeling (DATM) Tutorial \n",
    "\n",
    "## Part 2 of 2: Mapping Atoms to Text Data\n",
    "\n",
    "* This code is written in Python 3.7.2, and uses Gensim version 3.8.3. \n",
    "\n",
    "* This code is provides an the outline of how we assigned atoms to our cleaned data, which we show how to identify in Part 1 of 2. Note that we cannot redistribute the data used in our paper \"Integrating Topic Modeling and Word Embedding\" in any form, and researchers must apply directly to the Centers for Disease Control and Prevention for access. Details on data access are provided in the paper. We add comments with tips for adapting this code to your data.  \n",
    "* In our case, the goal of this code is to take a given narrative, get rolling windows of contexts from this narrative, find the SIF sentence embedding from each rolling window, and match the SIF embedding onto the closest (by cosine similarity) atom in the Dictionary loaded in earler. The SIF embedding is the maximum a posteriori (MAP) estimate of what the atom is for that sentence. So we'll get out a rolling window (i.e., a sequence) of atoms underlying the narrative.\n",
    "* In our case, we get atoms separately for law enforcement (narle) and medical examiner (narcme) narratives, and then combine the two distributions, as described in our paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from random import seed, sample\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used?\n",
    "from __future__ import division\n",
    "import cython\n",
    "import math\n",
    "from collections import Counter\n",
    "from ksvd import ApproximateKSVD \n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.linalg import norm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_a = 0.0001 #reasonable range for weight a is .001 to .0001 based on Arora et al SIF embeddings. The extent to which we re-weight words is controlled by the parameter $a$, where a lower value for $a$ means that frequent words are more aggressively down-weighted compared to less frequent words. \n",
    "seed_number = 5\n",
    "n_sample = 100000 #adjusting here to corpus sample, but consider using full corpus for final SIF embeddings. \n",
    "window_size = 7 # From chosen word embedding model\n",
    "vector_size = 300 # From chosen word embedding model\n",
    "min_tokens = 19 #this is set so that only narratives with at least 19 tokens in the w2v model vocab are considered. \n",
    "\n",
    "atoms_npz = \"./data/ngram_atoms.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in a word embedding model trained on the corpus (in our case, violent death narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel=Word2Vec.load('./models/gensim_model_window7_vector_300')\n",
    "#w2vmodel.init_sims(replace=False) #normalize word-vector lengths. May speed up if set replace=True, but then can't go back to the original (non-normalized) word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in dictionary of atoms identified in your word embedding (see \"DATM Tutorial Part 1 of 2\" for code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load back in the pickled dictionary of atom vectors\n",
    "#infile = open('./datm/250comp5nonzeros_dictionary','rb')\n",
    "#mydictionary=pickle.load(infile)\n",
    "#infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datm_ngrams/250comp5nonzeros_dictionary','rb') as f:\n",
    "    mydictionary=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare two pieces of informatin from the text data, which we will need to compute SIF Sentence Embeddings (MAP) of any given sentence \n",
    "\n",
    "* SIF Sentence Embedding is from: \"A Simple but tough-to-beat baseline for sentence embedding\" https://github.com/PrincetonML/SIF\n",
    "* To do SIF embeddings, we need to prep functions and two pieces of information from the raw text: (1) frequency weights for each word, and (2) the \"common discourse vector\" ($C_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ngram_comments.p', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "#this is the text data which has already been turned into trigrams and bigrams, cleaned, and tokenized. It is a list of lists, where each record is a word tokenized \"document\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The first input to SIF embeddings is an estimate of the frequency weights (based on probabilites) for each word in the corpus. Compute this here.**\n",
    "* This will naturally downweight stopwords when we compute a sentence embedding. It requires the raw text data of the corpus. \n",
    "* Either train a dictonary of weights (1), or upload a saved dictionary (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_dict(w2vmodel, weight_a=.0001): \n",
    "    freq_dictionary = {word: w2vmodel.wv.get_vecattr(word, \"count\") for word in w2vmodel.wv.index_to_key} \n",
    "    total= sum(freq_dictionary.values())\n",
    "    freq_dictionary = {word: weight_a/(weight_a + (w2vmodel.wv.get_vecattr(word, \"count\") / total)) for word in w2vmodel.wv.index_to_key} #best values according to arora et al are between .001 and .0001\n",
    "    return(freq_dictionary)\n",
    "\n",
    "#function to yield a weighted sentence, using the above weight dictionary\n",
    "def get_weighted_sent(tokedsent,freq_dict, w2vmodel=w2vmodel): \n",
    "    weightedsent= [freq_dict[word]*w2vmodel.wv[word] for word in tokedsent if word in freq_dict.keys()]\n",
    "    return(sum(weightedsent)/len(weightedsent)) #weightedsent_avg  #divide the weighted average by the number of words in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict= get_freq_dict(w2vmodel, weight_a)\n",
    "# What weight_a to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The second input to SIF embeddings is $C_0$, the common discourse vector. Compute this here.**\n",
    "* Get this with a random sample of discourse vectors since the data is so large, or compute using all narratives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samp_cts(docs, n_sample, windowsize, freq_dictionary):\n",
    "    sampnarrs=  sample(docs, n_sample) #sample of narratives. Will take 1 random window and discourse vector of this window, from each narrative. \n",
    "    sampvecs= []\n",
    "\n",
    "\n",
    "    t1_start = perf_counter()  \n",
    "\n",
    "    for i in sampnarrs: \n",
    "        if len(i)>windowsize: #want window length to be at least windowsize words\n",
    "            n= sample(range(0,len(i)-windowsize), 1)[0] #get some random positon in the narrative (at least windowsize steps behind the last one though)\n",
    "            sent= i[n:n+windowsize] #random context window \n",
    "            sampvecs.append(get_weighted_sent(i, freq_dictionary)) #sample a discourse vector, and append to a list of sample discourse vectors.\n",
    "            n= sample(range(0,len(i)-windowsize), 1)[0] #get some random positon in the narrative (at least windowsize steps behind the last one though)\n",
    "            sent= i[n:n+windowsize] #random context window \n",
    "            sampvecs.append(get_weighted_sent(i, freq_dictionary)) #sample a discourse vector, and append to a list of sample discourse vectors.\n",
    "    sampvecs= np.asarray(sampvecs)\n",
    "    t1_stop = perf_counter() #for 100k context windows takes  \n",
    "    print(t1_stop-t1_start)\n",
    "    return(sampvecs)\n",
    "\n",
    "def get_c0(sampvecs):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=10, random_state=0) #only keeping top component, using same method as in SIF embedding code\n",
    "    svd.fit(sampvecs) #1st singular vector  is now c_o\n",
    "    return(svd.components_[0])\n",
    "\n",
    "def remove_c0(comdiscvec, modcontextvecs):\n",
    "    curcontextvec= [X - X.dot(comdiscvec.transpose()) * comdiscvec for X in modcontextvecs] #remove c_0 from all the cts\n",
    "    curcontextvec=np.asarray(modcontextvecs)\n",
    "    return(curcontextvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.336826900000233\n"
     ]
    }
   ],
   "source": [
    "# Why a sample, why 50,000? Should I do 100,000 since I only do one corpus?\n",
    "\n",
    "seed(seed_number)\n",
    "sampvecs2_narcme= samp_cts(corpus, n_sample, window_size, freq_dict)\n",
    "#sampvecs2_narcme= samp_cts(sample(corpus, 50000), 50000, 10, freq_dict) #we used a random sample of 50,000 context vectors\n",
    "sampvecs2_narcme= normalize(sampvecs2_narcme, axis=1) #l2 normalize the resulting context vectors\n",
    "\n",
    "#seed(6)\n",
    "\n",
    "#sampvecs2_narle= samp_cts(sample(corpus, 50000), 50000, 10, freq_dict) #we used random sample of 50,000 context vectors\n",
    "#sampvecs2_narle= normalize(sampvecs2_narle, axis=1) #l2 normalize the resulting context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc0_narcme= get_c0(sampvecs2_narcme)\n",
    "#pc0_narle= get_c0(sampvecs2_narle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampvecs2_narcme = remove_c0(pc0_narcme, sampvecs2_narcme) \n",
    "#sampvecs2_narle = remove_c0(pc0_narle, sampvecs2_narle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Resulting function to get SIF MAPs along rolling windows, for a given narrative. \n",
    "\n",
    "* This the function we use to find rolling windows and assign MAPs to them, for a given narrative.\n",
    "* Note that this is set for our embedding size, which was 200-dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sif_atom_seqs(toked_narrative, window_size, vector_size, topics_dictionary, c0_vector, freq_dict, w2vmodel): \n",
    "    \n",
    "    toked_narr2 = [i for i in toked_narrative if i in w2vmodel.wv.index_to_key] #remove words not in vocab\n",
    "    if len(toked_narr2)> min_tokens :  \n",
    "        it = iter(toked_narr2) \n",
    "        win = [next(it) for cnt in range(0,window_size)] #first context window\n",
    "        MAPs= normalize(remove_c0( c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,vector_size))) #doing the SIF map here. Hardcoding in the dimensionality of the space to speed this up.\n",
    "        for e in it: # Subsequent windows\n",
    "            win[:-1] = win[1:]\n",
    "            win[-1] = e\n",
    "            MAPs = np.vstack((MAPs, normalize(remove_c0(c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,vector_size)))))  #this will be matrix of MAPs\n",
    "\n",
    "        costri= linear_kernel(MAPs, topics_dictionary) \n",
    "        atomsseq= np.argmax(costri, axis=1) #this is for the index of the closest atom to each of the MAPs\n",
    "        #maxinRow = np.amax(costri, axis=1) #this is for the closest atom's cossim value to each of the maps\n",
    "        return(atomsseq.tolist()) #returns sequence of the closest atoms to the MAPs\n",
    "    else:\n",
    "        return(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample usage of sif_atom_seqs, on a single narrative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Documents in a Corpus into a Sequences of Atoms \n",
    "\n",
    "* This is the **final result** we want from all code above\n",
    "* First, get c0 from narcme narratives, and then get the atom sequence for the narcme narratives\n",
    "* Then, get c0 from narle narratives, and then get the atom sequence for the narle narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get SIF Atom Seqs on NARCME narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2118317/2118317 [2:51:33<00:00, 205.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# This took about 3 hours for 2.1 million comments\n",
    "\n",
    "atom_seq = []\n",
    "for x in tqdm(corpus):\n",
    "    x = sif_atom_seqs(x, window_size, vector_size, mydictionary , pc0_narcme, freq_dict, w2vmodel)\n",
    "    if not(x == None):\n",
    "        x = ' '.join([str(elem) for elem in x])\n",
    "    else:\n",
    "        x = ''\n",
    "    \n",
    "    atom_seq.append(x)\n",
    "\n",
    "#df['narcme_atom_seq']= df['narcme_toked'].apply(lambda x: sif_atom_seqs(x, 10, mydictionary , pc0_narcme, freq_dict, w2vmodel) )\n",
    "\n",
    "#convert the atom seq to a string of atoms, since this format is needed for the vectorizer (and easier to work with later) in a CSV, too\n",
    "#df['narcme_atom_seq'] = df['narcme_atom_seq'].apply(lambda x: ' '.join([str(elem) for elem in x])  if(np.all(pd.notnull(x)))  else x ) #https://thispointer.com/python-how-to-convert-a-list-to-string/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20 20 144 144 64 7 7 180 180 64 180 180 239 239 239 137 137 137 228 228 228 228 236 236 157 72 157 157 157 157 157 177 177 177 177 177 71 71 71 71 71 71 71 154 154 154 129 129 129 83 14 14 14 14 14 14 14 244 244 121 121 121 73 73 73 73 73 73 73 134 134 134 134 134 134 134 134 210 210 210 210 210 173 195 195 195 19 111 19 203 203 122 122 122 122 122 111 111 240 236 236 236 236 236 236 17 71 71 71 17 17 17 223 223 223 223 120 120 120 120 68 68 10 10 64 64 101 101 37 37 37 197 197 197 197 204 165 165 240 17 17 17 17 125 125 125 219 219 240 240 240 23 240 128 23 128 128 128 125 125 125 10 37 37 37 37 224 224 224 78 78 88 88 88 88 88 23 23 23 23 23 23 23 78 103 135 135 125 125 125 138 138 224 224 240 11 11 240 75 75 75 75 75 14 75 14 14 14 14 14'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_seq[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting the Resulting Atom Sequences into Variables, by Vectorizing the Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform each sequence into a distribution over topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = TfidfVectorizer(analyzer = 'word', norm='l1', use_idf=False, token_pattern='\\S+') #need token pattern, otherwise splits using using 0-9 single digits too! #note that atoms that are part of all or no documents will not be transformed here, can reset this default, but I left as is for now since makes prediction easier (fewer features). #includes l1 normalization so that longer documents don't get more weight, l1 normalizes with abs value but all our values are pos anyways\n",
    "#bow_transformer.fit([x for x in atom_seq if x != '']) #corpus needs to be in format ['word word word'], NOT tokenized already\n",
    "bow_transformer.fit(atom_seq)\n",
    "\n",
    "vecked = bow_transformer.transform(atom_seq).toarray() #consider instead:  vecked = bow_transformer.transform(corpus['narcme_narle_atom_seq_combined'].dropna(inplace=True).tolist()).toarray() #this is the \"feature\" data, now in an array for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vecked, columns = bow_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"but she was n't either she was part fish and had no_idea her scars were actually gills she dreamed of water she was found as a baby near the water alone this leads me to believe that she was some_sort of hybrid between man and the asset 's species\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(corpus[4])\n",
    "# visual inspection confirms that current topic names (i.e., 1-250 match the discourse atoms)\n",
    "# For example, corpus[4] scores strongly on topic 94, which is she/her and a bunch of female first names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1        10  100       101  102       103      104  105  106  ...  \\\n",
       "0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.00000  0.0  0.0  ...   \n",
       "1  0.0  0.0  0.000000  0.0  0.000000  0.0  0.000000  0.00000  0.0  0.0  ...   \n",
       "2  0.0  0.0  0.014851  0.0  0.009901  0.0  0.004950  0.00495  0.0  0.0  ...   \n",
       "3  0.0  0.0  0.014286  0.0  0.009524  0.0  0.004762  0.00000  0.0  0.0  ...   \n",
       "4  0.0  0.0  0.000000  0.0  0.046512  0.0  0.000000  0.00000  0.0  0.0  ...   \n",
       "\n",
       "    90   91   92   93   94   95   96   97   98   99  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save it as a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = csr_matrix(df.astype(pd.SparseDtype(\"float\", 0)).sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2118317x250 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16901474 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(atoms_npz, df_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
