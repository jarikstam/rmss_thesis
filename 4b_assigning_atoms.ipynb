{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: copied from: https://github.com/arsena-k/discourse_atoms/blob/master/DATM%20Tutorial%20Part%202%20of%202%20-%20Assigning%20Atoms%20-%20for%20Public.ipynb\n",
    "\n",
    "My notes (18-02-2022):\n",
    "- I had to update the code for Gensim 4.0, which I think I did correctly (otherwise it probably wouldn't work)\n",
    "- I also didn't quite understand what datatype their corpus was. I thought it was lists of tokens nested inside a list of comments, but it seems it was actually a dataframe. From past experience I think pandas dataframes aren't very good at being iterated over, so I avoided using dataframes as much as possible. Only at the end I create a dataframe, but even then I think it's better to store it as a sparse matrix rather than a csv file.\n",
    "- Also since I'm not combining two datasets as they do, I skipped half the parts they do double. \n",
    "- In the end it seems every went correctly, but tbh I'm in over my head so I am not certain\n",
    "\n",
    "My notes (17-3-2022):\n",
    "- I changed the corpus to a gensim LineSentence style (i.e., a text file with each comment on a line and each term seperated by a whitespace\n",
    "- When I tried to include all comments (instead of a sample), I was getting a few (2 in 2 million) weighted sents of length zero, which caused a divide by zero error. I'm not sure how this is possible\n",
    "- I also don't understand why there were 2 samples taken per comment in the sample_cts function. They way it was coded (the same 3 lines copy/pasted below each other) makes it seem unintentional. I only take nne per comment\n",
    "- Furthermore, I realized that this windowsize doesn't need to be the same as that of the word embedding vector.\n",
    "\n",
    "\n",
    "# Discourse Atom Topic Modeling (DATM) Tutorial \n",
    "\n",
    "## Part 2 of 2: Mapping Atoms to Text Data\n",
    "\n",
    "* This code is written in Python 3.7.2, and uses Gensim version 3.8.3. \n",
    "\n",
    "* This code is provides an the outline of how we assigned atoms to our cleaned data, which we show how to identify in Part 1 of 2. Note that we cannot redistribute the data used in our paper \"Integrating Topic Modeling and Word Embedding\" in any form, and researchers must apply directly to the Centers for Disease Control and Prevention for access. Details on data access are provided in the paper. We add comments with tips for adapting this code to your data.  \n",
    "* In our case, the goal of this code is to take a given narrative, get rolling windows of contexts from this narrative, find the SIF sentence embedding from each rolling window, and match the SIF embedding onto the closest (by cosine similarity) atom in the Dictionary loaded in earler. The SIF embedding is the maximum a posteriori (MAP) estimate of what the atom is for that sentence. So we'll get out a rolling window (i.e., a sequence) of atoms underlying the narrative.\n",
    "* In our case, we get atoms separately for law enforcement (narle) and medical examiner (narcme) narratives, and then combine the two distributions, as described in our paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from random import seed, sample\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used?\n",
    "from __future__ import division\n",
    "import cython\n",
    "import math\n",
    "from collections import Counter\n",
    "from ksvd import ApproximateKSVD \n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.linalg import norm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "chosen_wv_model = './models/gensim_model_window25_vector300'\n",
    "chosen_datm_model = './datm/325comp_5nonzeros_dictionary'\n",
    "# ngram_comments_file = './data/ngram_comments.p' #del\n",
    "tokenized_comments_v2_file = './data/tokenized_comments' #del\n",
    "tokenized_comments_text_file = './data/tokenized_comments.txt'\n",
    "\n",
    "# Export atom loadings per comment\n",
    "# I thought this window size had to be the same as that for word embedding, but it doesn't\n",
    "# A-K et al choose a word embedding with window size 5, but had a window size 10 for datm\n",
    "atoms_npz = \"./data/atoms.npz\" #this was with window_size =25 (meaning only comments of over 50 words were looked at)\n",
    "atoms_window5_npz = \"./data/atoms_window5.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_a = 0.0001 #reasonable range for weight a is .001 to .0001 based on Arora et al SIF embeddings. The extent to which we re-weight words is controlled by the parameter $a$, where a lower value for $a$ means that frequent words are more aggressively down-weighted compared to less frequent words. \n",
    "seed_number = 5\n",
    "\n",
    "# n_sample currently ignored because looping over full corpus.\n",
    "n_sample = 100000 #adjusting here to corpus sample, but consider using full corpus for final SIF embeddings. \n",
    "window_size = 5 # From chosen word embedding model ?? or not\n",
    "vector_size = 300 # From chosen word embedding model\n",
    "min_tokens = window_size*2-1 #this is set so that only narratives with at least 19 tokens in the w2v model vocab are considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel=Word2Vec.load(chosen_wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chosen_datm_model,'rb') as f:\n",
    "    mydictionary=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mydictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare two pieces of informatin from the text data, which we will need to compute SIF Sentence Embeddings (MAP) of any given sentence \n",
    "\n",
    "* SIF Sentence Embedding is from: \"A Simple but tough-to-beat baseline for sentence embedding\" https://github.com/PrincetonML/SIF\n",
    "* To do SIF embeddings, we need to prep functions and two pieces of information from the raw text: (1) frequency weights for each word, and (2) the \"common discourse vector\" ($C_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = LineSentence(tokenized_comments_text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The first input to SIF embeddings is an estimate of the frequency weights (based on probabilites) for each word in the corpus. Compute this here.**\n",
    "* This will naturally downweight stopwords when we compute a sentence embedding. It requires the raw text data of the corpus. \n",
    "* Either train a dictonary of weights (1), or upload a saved dictionary (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_dict(w2vmodel, weight_a=.0001): \n",
    "    freq_dictionary = {word: w2vmodel.wv.get_vecattr(word, \"count\") for word in w2vmodel.wv.index_to_key} \n",
    "    total= sum(freq_dictionary.values())\n",
    "    freq_dictionary = {word: weight_a/(weight_a + (w2vmodel.wv.get_vecattr(word, \"count\") / total)) for word in w2vmodel.wv.index_to_key} #best values according to arora et al are between .001 and .0001\n",
    "    return(freq_dictionary)\n",
    "\n",
    "#function to yield a weighted sentence, using the above weight dictionary\n",
    "def get_weighted_sent(tokedsent,freq_dict, w2vmodel=w2vmodel): \n",
    "    weightedsent= [freq_dict[word]*w2vmodel.wv[word] for word in tokedsent if word in freq_dict.keys()]\n",
    "    if len(weightedsent) != 0:\n",
    "        return(sum(weightedsent)/len(weightedsent)) #weightedsent_avg  #divide the weighted average by the number of words in the sentence\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict= get_freq_dict(w2vmodel, weight_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The second input to SIF embeddings is $C_0$, the common discourse vector. Compute this here.**\n",
    "* Get this with a random sample of discourse vectors since the data is so large, or compute using all narratives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use all docs instead of a sample\n",
    "# I added a if for sampvecs.append because otherwise sometimes divided by zero. Note: also changed to code for get_weighted_sent\n",
    "# also why are there two samples per doc???\n",
    "\n",
    "def samp_cts(docs, windowsize, freq_dictionary, vector_size):\n",
    "    #sampnarrs=  sample(docs, n_sample) #sample of narratives. Will take 1 random window and discourse vector of this window, from each narrative. \n",
    "    sampvecs= []\n",
    "\n",
    "    for i in tqdm(docs): \n",
    "        if len(i)>windowsize: #want window length to be at least windowsize words\n",
    "            n= sample(range(0,len(i)-windowsize), 1)[0] #get some random positon in the narrative (at least windowsize steps behind the last one though)\n",
    "            sent= i[n:n+windowsize] #random context window \n",
    "            weighted_sent = get_weighted_sent(i, freq_dictionary)\n",
    "            if weighted_sent is not None: # length zero\n",
    "                sampvecs.append(weighted_sent) #sample a discourse vector, and append to a list of sample discourse vectors.\n",
    "    sampvecs= np.asarray(sampvecs)\n",
    "\n",
    "    return(sampvecs)\n",
    "\n",
    "def get_c0(sampvecs):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=10, random_state=0) #only keeping top component, using same method as in SIF embedding code\n",
    "    svd.fit(sampvecs) #1st singular vector  is now c_o\n",
    "    return(svd.components_[0])\n",
    "\n",
    "def remove_c0(comdiscvec, modcontextvecs):\n",
    "    curcontextvec= [X - X.dot(comdiscvec.transpose()) * comdiscvec for X in modcontextvecs] #remove c_0 from all the cts\n",
    "    curcontextvec=np.asarray(modcontextvecs)\n",
    "    return(curcontextvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2114724it [04:48, 7327.97it/s] \n"
     ]
    }
   ],
   "source": [
    "# With 100,000 sample took about 30 secs\n",
    "# With full corpus (2mil comments) took 9 minutes\n",
    "\n",
    "sampvecs2_narcme= samp_cts(corpus, window_size, freq_dict, vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampvecs2_narcme= normalize(sampvecs2_narcme, axis=1) #l2 normalize the resulting context vectors\n",
    "\n",
    "pc0_narcme= get_c0(sampvecs2_narcme)\n",
    "sampvecs2_narcme = remove_c0(pc0_narcme, sampvecs2_narcme) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c, i in enumerate(sampvecs2_narcme):\n",
    "    if i is None:\n",
    "        #print(len(i))\n",
    "        print(f\"Position {c}\")\n",
    "        print(i)\n",
    "        print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948627"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampvecs2_narcme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Resulting function to get SIF MAPs along rolling windows, for a given narrative. \n",
    "\n",
    "* This the function we use to find rolling windows and assign MAPs to them, for a given narrative.\n",
    "* Note that this is set for our embedding size, which was 200-dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sif_atom_seqs(toked_narrative, window_size, vector_size, topics_dictionary, c0_vector, freq_dict, w2vmodel): \n",
    "    \n",
    "    toked_narr2 = [i for i in toked_narrative if i in w2vmodel.wv.index_to_key] #remove words not in vocab\n",
    "    if len(toked_narr2)> min_tokens :  \n",
    "        it = iter(toked_narr2)\n",
    "        win = [next(it) for cnt in range(0,window_size)] #first context window\n",
    "        MAPs= normalize(remove_c0( c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,vector_size))) #doing the SIF map here. Hardcoding in the dimensionality of the space to speed this up.\n",
    "        for e in it: # Subsequent windows\n",
    "            win[:-1] = win[1:]\n",
    "            win[-1] = e\n",
    "            MAPs = np.vstack((MAPs, normalize(remove_c0(c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,vector_size)))))  #this will be matrix of MAPs\n",
    "\n",
    "        costri= linear_kernel(MAPs, topics_dictionary) \n",
    "        atomsseq= np.argmax(costri, axis=1) #this is for the index of the closest atom to each of the MAPs\n",
    "        #maxinRow = np.amax(costri, axis=1) #this is for the closest atom's cossim value to each of the maps\n",
    "        return(atomsseq.tolist()) #returns sequence of the closest atoms to the MAPs\n",
    "    else:\n",
    "        return(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Documents in a Corpus into a Sequences of Atoms \n",
    "\n",
    "* This is the **final result** we want from all code above\n",
    "* First, get c0 from narcme narratives, and then get the atom sequence for the narcme narratives\n",
    "* Then, get c0 from narle narratives, and then get the atom sequence for the narle narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get SIF Atom Seqs on NARCME narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2116273/2116273 [3:10:06<00:00, 185.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# This took about 3 hours for 2.1 million comments\n",
    "\n",
    "atom_seq = []\n",
    "with open(tokenized_comments_text_file, 'r') as f:\n",
    "    \n",
    "    for x in tqdm(f.readlines()):\n",
    "        x = x.replace(\"\\n\", \"\").split(\" \")\n",
    "        x = sif_atom_seqs(x, window_size, vector_size, mydictionary , pc0_narcme, freq_dict, w2vmodel)\n",
    "        if not(x == None):\n",
    "            x = ' '.join([str(elem) for elem in x])\n",
    "        else:\n",
    "            x = ''\n",
    "\n",
    "        atom_seq.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting the Resulting Atom Sequences into Variables, by Vectorizing the Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform each sequence into a distribution over topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = TfidfVectorizer(analyzer = 'word', norm='l1', use_idf=False, token_pattern='\\S+') #need token pattern, otherwise splits using using 0-9 single digits too! #note that atoms that are part of all or no documents will not be transformed here, can reset this default, but I left as is for now since makes prediction easier (fewer features). #includes l1 normalization so that longer documents don't get more weight, l1 normalizes with abs value but all our values are pos anyways\n",
    "#bow_transformer.fit([x for x in atom_seq if x != '']) #corpus needs to be in format ['word word word'], NOT tokenized already\n",
    "bow_transformer.fit(atom_seq)\n",
    "\n",
    "vecked = bow_transformer.transform(atom_seq).toarray() #consider instead:  vecked = bow_transformer.transform(corpus['narcme_narle_atom_seq_combined'].dropna(inplace=True).tolist()).toarray() #this is the \"feature\" data, now in an array for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vecked, columns = bow_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save it as a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = csr_matrix(df.astype(pd.SparseDtype(\"float\", 0)).sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2116273x325 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 23672877 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(atoms_window5_npz, df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>...</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>8</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>9</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1     10       100       101  102  103  104  105  106       107  \\\n",
       "0  0.0  0.0  0.000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "1  0.0  0.0  0.000  0.000000  0.058824  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "2  0.0  0.0  0.000  0.004902  0.009804  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "3  0.0  0.0  0.000  0.000000  0.009434  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "4  0.0  0.0  0.000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "5  0.0  0.0  0.625  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "6  0.0  0.0  0.000  0.000000  0.050000  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "7  0.0  0.0  0.000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "8  0.0  0.0  0.000  0.000000  0.019608  0.0  0.0  0.0  0.0  0.0  0.019608   \n",
       "9  0.0  0.0  0.000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.000000   \n",
       "\n",
       "        108  109   11       110       111  112       113  114  115       116  \\\n",
       "0  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
       "1  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.176471   \n",
       "2  0.029412  0.0  0.0  0.000000  0.024510  0.0  0.009804  0.0  0.0  0.029412   \n",
       "3  0.018868  0.0  0.0  0.000000  0.023585  0.0  0.009434  0.0  0.0  0.018868   \n",
       "4  0.000000  0.0  0.0  0.066667  0.000000  0.0  0.000000  0.0  0.0  0.044444   \n",
       "5  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
       "6  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
       "7  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
       "8  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
       "9  0.000000  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
       "\n",
       "   117  118       119   12  120       121  122       123   124  ...   72   73  \\\n",
       "0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.00  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.00  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.034314  0.0  0.0  0.000000  0.0  0.000000  0.00  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.033019  0.0  0.0  0.004717  0.0  0.009434  0.00  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.00  ...  0.0  0.0   \n",
       "5  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.00  ...  0.0  0.0   \n",
       "6  0.0  0.0  0.016667  0.0  0.0  0.000000  0.0  0.000000  0.05  ...  0.0  0.0   \n",
       "7  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.00  ...  0.0  0.0   \n",
       "8  0.0  0.0  0.039216  0.0  0.0  0.000000  0.0  0.049020  0.00  ...  0.0  0.0   \n",
       "9  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.000000  0.00  ...  0.0  0.0   \n",
       "\n",
       "    74   75   76   77        78   79    8        80   81   82        83   84  \\\n",
       "0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.039216  0.0  0.0  0.000000  0.0  0.0  0.014706  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.037736  0.0  0.0  0.000000  0.0  0.0  0.014151  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.033333  0.0  0.0  0.000000  0.0   \n",
       "7  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.058824  0.0  0.0  0.000000  0.0   \n",
       "9  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "\n",
       "         85        86        87   88   89    9        90        91   92  \\\n",
       "0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "1  0.000000  0.000000  0.058824  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "2  0.004902  0.009804  0.004902  0.0  0.0  0.0  0.014706  0.014706  0.0   \n",
       "3  0.009434  0.009434  0.000000  0.0  0.0  0.0  0.014151  0.014151  0.0   \n",
       "4  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "5  0.000000  0.125000  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "6  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "7  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "8  0.009804  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "9  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.000000  0.0   \n",
       "\n",
       "         93   94   95   96   97   98   99  \n",
       "0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8  0.019608  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 325 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additionally thank_you so much for caring enough to ask in a genuinely_curious and respectful way i 'm_curious to hear your thoughts\n",
      "\n",
      "i am watching shape of water it says some stuff in russian how can i find out what they are saying\n",
      "\n",
      "extremely underwhelmed by the film it just felt dull the 'romance between fish-dick and mute-chick was too quick and i never felt any connection or empathy for the 'asset the pie-shop scenes were so quick and short and ultimately meant nothing i want to get to know you ew no and that was it just the film saying hah look the 40 's hated gays the strange musical imagination scene was a complete tonal change from the rest of the film and it seems the film just ignored all logic to tell the story the 'asset had no security at all no guards no cameras the cleaner was allowed in and out whenever she felt like it most of the time when the guy 's hand was injured it took ages for any help to arrive in a top-security lab a cleaner and her elderly friend manage to break out the creature in a top-security lab as well and there was only one guard in the entire place apparently they pretty_much just walk it out the back door and no one gives chase it just felt completely off and i 'll ignore the bathroom that was strong_enough to fill up with water which completely_disregarded physics the film was utterly inconsistent and did nothing memorable or new\n",
      "\n",
      "we need a quirky 'mute girl we need a gay guy we need a black sidekick we need a fully evil white guy in a suit we need the russians to try to fuck everything up it does seem to tick all the boxes i also hated the movie but not for those reasons it just felt dull the 'romance between fish-dick and mute-chick was too quick and i never felt any connection or empathy for the 'asset the strange musical imagination scene was a complete tonal change from the rest of the film and it seems the film just ignored all logic to tell the story the 'asset had no security at all no guards no cameras the cleaner was allowed in and out whenever she felt like it most of the time when the guy 's hand was injured it took ages for any help to arrive in a top-security lab a cleaner and her elderly friend manage to break out the creature in a top-security lab as well and there was only one guard in the entire place apparently they pretty_much just walk it out the back door it just felt completely off and i 'll ignore the bathroom that was strong_enough to fill up with water which completely_disregarded physics the film was completely inconsistent and did nothing memorable or new\n",
      "\n",
      "but she was n't either she was part fish and had no_idea her scars were actually gills she dreamed of water she was found as a baby near the water alone this leads me to believe that she was some_sort of hybrid between man and the asset 's species\n",
      "\n",
      "i thought it was a great metaphor he was given the ol fishhook\n",
      "\n",
      "i feel this but mostly fuck that scene that suggested you can fill a fucking bathroom with 8 feet of water by blocking the bottom of the door with a towel seriously that was ridiculous and then the folks in the theater below like barely getting dripped on please that is literally an elephants weight worth of water that whole apartment is coming down\n",
      "\n",
      "i just watched the movie and i dont get this mind explaining\n",
      "\n",
      "i get what you_'re saying and it makes_sense but it still seems very out of character to me he was a character who was determined and successful in eliminating the threat the simple fact that the foe did n't go down as expected should n't have_been enough to change his perspective i would have expected rage and continued competency instead of him just folding i mean how did he ever even capture the creature if he just lets it kill him like that at the last_minute plus as i mentioned before i think it would have_been super hard to see if those wounds were really healed\n",
      "\n",
      "hahahaha fuckkkkkkk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "for i in corpus:\n",
    "    x+=1\n",
    "    print(\" \".join(i))\n",
    "    print()\n",
    "    if x == 10:\n",
    "        break\n",
    "# visual inspection confirms that current topic names (i.e., 1-250 match the discourse atoms)\n",
    "# For example, corpus[4] scores strongly on topic 94, which is she/her and a bunch of female first names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
