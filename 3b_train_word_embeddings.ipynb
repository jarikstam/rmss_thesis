{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "african-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "important-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file=\"./data/tokens.txt\"\n",
    "model_path=\"./models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-insurance",
   "metadata": {},
   "source": [
    "## Train and test embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What Arseniev-Koehler et al. (2021) do (see their appendix 2):\n",
    "# Also: CBOW, negative sampling (with negative=5)\n",
    "window_size = [5,7,10]\n",
    "vector_size = [50,100,200,300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-crowd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for window in window_size:\n",
    "    for vector in vector_size:\n",
    "        name = f\"gensim_model_window{window}_vector_{vector}\"\n",
    "        print(f\"Starting with {name} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        model = Word2Vec(corpus_file=corpus_file, vector_size=vector, window=window, epochs=10, min_count=15)\n",
    "\n",
    "        google_test = model.wv.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "        print(google_test[0])\n",
    "        print(google_test[1][-1])\n",
    "        similarities = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "        print(similarities)\n",
    "        print()\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        filename=f\"{model_path}{name}\"\n",
    "        model.save(filename)\n",
    "    \n",
    "print(f\"Finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-large",
   "metadata": {},
   "source": [
    "## Detect ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "color-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn ngrams\n",
    "sentences = gensim.models.word2vec.LineSentence(corpus_file)\n",
    "ngrams = Phrases(sentences, connector_words=ENGLISH_CONNECTOR_WORDS, min_count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "searching-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenized_comments.p', 'rb') as f:\n",
    "    comments = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-spanking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['additionally',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'so',\n",
       " 'much',\n",
       " 'for',\n",
       " 'caring',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'in',\n",
       " 'a',\n",
       " 'genuinely',\n",
       " 'curious',\n",
       " 'and',\n",
       " 'respectful',\n",
       " 'way',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'curious',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'your',\n",
       " 'thoughts']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crucial-template",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['additionally',\n",
       " 'thank_you',\n",
       " 'so',\n",
       " 'much',\n",
       " 'for',\n",
       " 'caring',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'in',\n",
       " 'a',\n",
       " 'genuinely_curious',\n",
       " 'and',\n",
       " 'respectful',\n",
       " 'way',\n",
       " 'i',\n",
       " \"'m_curious\",\n",
       " 'to',\n",
       " 'hear',\n",
       " 'your',\n",
       " 'thoughts']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams[comments[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "theoretical-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2118317/2118317 [10:07<00:00, 3488.65it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_comments = []\n",
    "\n",
    "for comment in tqdm(comments):\n",
    "    comment_tokens = ngrams[comment]\n",
    "    tokenized_comments.append(comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continent-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['additionally', 'thank_you', 'so', 'much', 'for', 'caring', 'enough', 'to', 'ask', 'in', 'a', 'genuinely_curious', 'and', 'respectful', 'way', 'i', \"'m_curious\", 'to', 'hear', 'your', 'thoughts'], ['i', 'am', 'watching', 'shape_of_water', 'it', 'says', 'some', 'stuff', 'in', 'russian', 'how', 'can', 'i', 'find', 'out', 'what', 'they', 'are', 'saying'], ['extremely', 'underwhelmed', 'by', 'the', 'film', 'it', 'just', 'felt', 'dull', 'the', \"'romance\", 'between', 'fish-dick', 'and', 'mute-chick', 'was', 'too', 'quick', 'and', 'i', 'never', 'felt', 'any', 'connection', 'or', 'empathy', 'for', 'the', \"'asset\", 'the', 'pie-shop', 'scenes', 'were', 'so', 'quick', 'and', 'short', 'and', 'ultimately', 'meant', 'nothing', 'i', 'want', 'to', 'get', 'to', 'know', 'you', 'ew', 'no', 'and', 'that', 'was', 'it', 'just', 'the', 'film', 'saying', 'hah', 'look', 'the', '40', \"'s\", 'hated', 'gays', 'the', 'strange', 'musical', 'imagination', 'scene', 'was', 'a', 'complete', 'tonal', 'change', 'from', 'the', 'rest', 'of', 'the', 'film', 'and', 'it', 'seems', 'the', 'film', 'just', 'ignored', 'all', 'logic', 'to', 'tell', 'the', 'story', 'the', \"'asset\", 'had', 'no', 'security', 'at', 'all', 'no', 'guards', 'no', 'cameras', 'the', 'cleaner', 'was', 'allowed', 'in', 'and', 'out', 'whenever', 'she', 'felt', 'like', 'it', 'most', 'of', 'the', 'time', 'when', 'the', 'guy', \"'s\", 'hand', 'was', 'injured', 'it', 'took', 'ages', 'for', 'any', 'help', 'to', 'arrive', 'in', 'a', 'top-security', 'lab', 'a', 'cleaner', 'and', 'her', 'elderly', 'friend', 'manage', 'to', 'break', 'out', 'the', 'creature', 'in', 'a', 'top-security', 'lab', 'as', 'well', 'and', 'there', 'was', 'only', 'one', 'guard', 'in', 'the', 'entire', 'place', 'apparently', 'they', 'pretty_much', 'just', 'walk', 'it', 'out', 'the', 'back', 'door', 'and', 'no', 'one', 'gives', 'chase', 'it', 'just', 'felt', 'completely', 'off', 'and', 'i', \"'ll\", 'ignore', 'the', 'bathroom', 'that', 'was', 'strong_enough', 'to', 'fill', 'up', 'with', 'water', 'which', 'completely_disregarded', 'physics', 'the', 'film', 'was', 'utterly', 'inconsistent', 'and', 'did', 'nothing', 'memorable', 'or', 'new'], ['we', 'need', 'a', 'quirky', \"'mute\", 'girl', 'we', 'need', 'a', 'gay', 'guy', 'we', 'need', 'a', 'black', 'sidekick', 'we', 'need', 'a', 'fully', 'evil', 'white', 'guy', 'in', 'a', 'suit', 'we', 'need', 'the', 'russians', 'to', 'try', 'to', 'fuck', 'everything', 'up', 'it', 'does', 'seem', 'to', 'tick', 'all', 'the', 'boxes', 'i', 'also', 'hated', 'the', 'movie', 'but', 'not', 'for', 'those', 'reasons', 'it', 'just', 'felt', 'dull', 'the', \"'romance\", 'between', 'fish-dick', 'and', 'mute-chick', 'was', 'too', 'quick', 'and', 'i', 'never', 'felt', 'any', 'connection', 'or', 'empathy', 'for', 'the', \"'asset\", 'the', 'strange', 'musical', 'imagination', 'scene', 'was', 'a', 'complete', 'tonal', 'change', 'from', 'the', 'rest', 'of', 'the', 'film', 'and', 'it', 'seems', 'the', 'film', 'just', 'ignored', 'all', 'logic', 'to', 'tell', 'the', 'story', 'the', \"'asset\", 'had', 'no', 'security', 'at', 'all', 'no', 'guards', 'no', 'cameras', 'the', 'cleaner', 'was', 'allowed', 'in', 'and', 'out', 'whenever', 'she', 'felt', 'like', 'it', 'most', 'of', 'the', 'time', 'when', 'the', 'guy', \"'s\", 'hand', 'was', 'injured', 'it', 'took', 'ages', 'for', 'any', 'help', 'to', 'arrive', 'in', 'a', 'top-security', 'lab', 'a', 'cleaner', 'and', 'her', 'elderly', 'friend', 'manage', 'to', 'break', 'out', 'the', 'creature', 'in', 'a', 'top-security', 'lab', 'as', 'well', 'and', 'there', 'was', 'only', 'one', 'guard', 'in', 'the', 'entire', 'place', 'apparently', 'they', 'pretty_much', 'just', 'walk', 'it', 'out', 'the', 'back', 'door', 'it', 'just', 'felt', 'completely', 'off', 'and', 'i', \"'ll\", 'ignore', 'the', 'bathroom', 'that', 'was', 'strong_enough', 'to', 'fill', 'up', 'with', 'water', 'which', 'completely_disregarded', 'physics', 'the', 'film', 'was', 'completely', 'inconsistent', 'and', 'did', 'nothing', 'memorable', 'or', 'new'], ['but', 'she', 'was', \"n't\", 'either', 'she', 'was', 'part', 'fish', 'and', 'had', 'no_idea', 'her', 'scars', 'were', 'actually', 'gills', 'she', 'dreamed', 'of', 'water', 'she', 'was', 'found', 'as', 'a', 'baby', 'near', 'the', 'water', 'alone', 'this', 'leads', 'me', 'to', 'believe', 'that', 'she', 'was', 'some_sort', 'of', 'hybrid', 'between', 'man', 'and', 'the', 'asset', \"'s\", 'species']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_comments[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reasonable-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams.save('./data/ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blond-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ngram_comments.p', 'wb') as f:\n",
    "    pickle.dump(tokenized_comments, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
