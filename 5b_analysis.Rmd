---
title: "CMD"
author: "Jarik Stam"
date: "4-3-2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(word2vec)
library(text2map)
library(Matrix)
library(data.table)
library(tidyverse)
library(lubridate)
library(lme4)
library(optimx)
library(xtable)
library(stargazer)
library(ggcorrplot)

theme_set(cowplot::theme_minimal_hgrid())
Sys.setlocale("LC_TIME", "English")
```
```{r files}
analysis_file <- "data/comments_analysis.csv"
topics_file <- "data/topics_annotated.csv"
tmdb_file <- "data/submissions_tmdb.csv"

figures_path <- "figures"

binary_wv_file <- "./models/gensim_model_window25_vector300_kv.w2v"
wvdf_file <- "data/wvdf.csv"
```
```{r functions}
se <- function(x) sd(x)/sqrt(length(x))

predict_submission <- function(model) {
  model_name <- deparse(substitute(model))
  ranef(model, condVar=F, whichel="submission_id") %>% 
    as_tibble() %>% 
    mutate({{ model_name }} := condval #+ fixef(model)[1]
           ) %>% 
    select(grp, {{ model_name }}) %>% 
    rename("submission_id" = grp)
}
```
```{r data}
# Importing comments and creating dtm ----
comments <- fread(analysis_file, data.table = FALSE)

dtm <- comments %>% 
  dtm_builder(tokenized_body, comment_id)

pol_names <- read_delim(topics_file, delim=";") %>% 
  filter(Category=="Politics") %>% 
  distinct(Name) %>% 
  pull()

movements <- c("BLM", "MeToo", "OscarsSoWhite")

comments <- as_tibble(comments) %>% 
  select(!c(created, tokenized_body)) %>% 
  # mutate(across(
  #   all_of(pol_names), ~ .x*100
  # ))
  select(!all_of(pol_names))

# Filtering comments ----
# AutoModerator is a bot used by Reddit moderators for automized moderation
# [deleted] and None are unknown authors (mostly) because of deleted accounts whose comments remain, meaning I can't test for within-author change
comments <- comments %>%
  filter(!author %in% c("AutoModerator", "[deleted]", "None"))

# Removing days from before min_date because before that data is very sparse
# From may 2013 there is at least on film discussion thread per month
# And from after Jan 2022 because data might be incomplete
# (i.e., those threads were still active at time of collection)
# This only removes ~ 10000 comments
min_date <- ymd("2013-05-01")

comments <- comments %>%
  filter(Date >= min_date,
         Date < ymd("2022-02-01")) %>%
  mutate(years = interval(min_date, Date) / years(1))

comments <- comments %>% 
  group_by(submission_id) %>% 
  mutate(thread_date = min(Date),
         thread_age = interval(min_date, min(Date)) / years(1)) %>% 
  ungroup() %>% 
  mutate(comment_age = as.integer(Date - thread_date))

# For a long time, one wouldn't be able to comment in submissions older than 180 days
# This was disabled 15-okt-2021, meaning there are now a few comments way older than 180 days (i.e., 3000 days), I think these comments are outliers
# See lines 291-292 from https://github.com/reddit-archive/reddit/commit/0ae8f2fb96cd39a01e8bff2cb4b1829b7bdbd0a8#diff-f28c2f2d93f455301ef0180437b545fdR291

comments <- comments %>% 
  filter(comment_age <= 180)

# Removing authors who haven't commented in at least 2 different submissions, because they add a lot of calculation without adding much interesting variation
comments <- comments %>% 
  group_by(author) %>% 
  filter(n_distinct(submission_id) > 1) %>% 
  # Doing log(x+1) because that's what I do with other logged vars
  mutate(user_comments = log(n()+1)) %>% 
  ungroup()

# Mutating variables ----
comments <- comments %>% 
  mutate(
    # Mutating to log(x+1) for several vars because they minimum zero and very long righthand tails
    across(all_of(c(movements, "politics_comments", "vote_count")), ~ log(.x+1)),
    # Standardizing score to make its range more similar to that of other vars
    across(c(score), ~ scale(.x)[,1]),
    # Converting to datetype
    Date = date(Date),
    # Creating a dummy because the var isn't very continuous
    across(ends_with("keywords"), ~ if_else(.x == 0, 0, 1))
    )

# Other data ----
# model <- read.word2vec(file = binary_wv_file, normalize = TRUE)

tmdb <- read_delim(tmdb_file, delim = ";")
genres <- tmdb %>% select(`Science Fiction`:Documentary) %>% colnames()

comments <- tmdb %>% 
  select(all_of(c("submission_id", genres))) %>% 
  right_join(comments)

wv <- read_csv(wvdf_file)
wv <- column_to_rownames(wv, "...1")
wv <- data.matrix(wv, rownames.force = TRUE)
```
```{r params}
corr_colors <- rev(RColorBrewer::brewer.pal(3, "RdYlBu"))

date_metoo <- ymd("2017-10-15")


```
```{r cmd}
# https://cran.r-project.org/web/packages/text2map/vignettes/CMDist-concept-movers-distance.html
# Options: single words, compound words, semantic directions, centroids.

# The problem with taking a single seed word is that grammar matters. If it's a plural noun, it more likely to find other plural nouns etc.
# That said, differences are small

concept_words <- c("metoo", "blm", "sexism", "racism", "discrimination")
sexism_words <- c("sexism", "metoo", "sexist", "sexual_harassment", "misogyny", "patriarchy",
                  "sexualization", "sjws", "rape_culture", "toxic_masculinity")
racism_words <- c("racism", "blm","racist", "racists", "african_americans",
                  "racial", "segregation", "systemic_racism", "police_brutality", "white_supremacy",
                  "institutional_racism", "race_relations", "bigoted")
discrimination_words <- c("discrimination", sexism_words, racism_words)

# predict(model, newdata = "racism", type = "nearest", top_n = 10)

# sexism_centroid <- get_centroid(sexism_words, wv)
# racism_centroid <- get_centroid(racism_words, wv)
discrimination_centroid <- get_centroid(discrimination_words, wv)

# doc_closeness <- CMDist(dtm = dtm, cw = concept_words, cv = sexism_centroid, wv = wv)
# racism_closeness <- CMDist(dtm = dtm, cv = racism_centroid, wv = wv)
discrimination_closeness <- CMDist(dtm = dtm, cv = discrimination_centroid, wv = wv)

# doc_closeness <- doc_closeness %>% 
#   rename_with(~ str_c(.x, "_cmd"),
#               all_of(concept_words)
#   )

comments <- comments %>% 
  # left_join(doc_closeness, by=c("comment_id" = "doc_id")) %>% 
  # left_join(racism_closeness, by=c("comment_id" = "doc_id")) %>% 
  left_join(discrimination_closeness, by=c("comment_id" = "doc_id"))

rm(dtm, doc_closeness, racism_closeness, discrimination_closeness, wv)
```
```{r descriptives}
# Number of discussion threads per month ----
comments %>% 
  group_by(submission_id, film_title) %>% 
  summarise(Date = min(Date)) %>%
  mutate(Date = floor_date(Date, "month")) %>% 
  ggplot(aes(Date)) + 
  geom_bar() + 
  scale_x_date(
    date_breaks = "2 months", date_labels = "%b '%y", 
    expand = expansion(mult = c(0, 0))) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# Effect of time ----
comments %>% 
  ggplot(aes(Date, sexism_centroid)) +
  geom_smooth(method="lm", se=T, color="red", fill="red") +
  geom_smooth(method="lm", formula = "y ~ poly(x, 2)", se=T, color="yellow", fill="yellow") +
  # geom_smooth(method="lm", formula = "y ~ poly(x, 3)", se=F, color="green") +
  # geom_smooth(method="lm", formula = "y ~ poly(x, 4)", se=F, color="blue") +
  geom_smooth(method="lm", formula = "y ~ poly(x, 5)", se=T, color="purple", fill="purple") +
  # scale_x_date(
  #   date_breaks = "1 years", date_labels = "%Y", 
  #   expand = expansion(mult = c(0, 0))) + 
    scale_x_date(date_breaks = "2 months", date_labels = "%b '%y",
               expand = expansion(mult = c(0, 0))) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# Descriptive statistics ----
comments %>% 
  select(where(is.numeric)) %>% 
  as.data.frame() %>% 
  stargazer(., nobs = FALSE, type = "text")

# Correlations ----
comments %>% 
  select(where(is.numeric)) %>%
  cor() %>% 
  ggcorrplot(lab=T, outline.col = "white",
             colors = corr_colors,
             show.legend = F, lab_size = 3.5,
             show.diag = F)

tmdb %>% 
  select(where(is.numeric)) %>%
  select(sexism_keywords, racism_keywords, everything()) %>% 
  cor() %>% 
  ggcorrplot(lab=T, outline.col = "white",
             colors = corr_colors,
             show.legend = F, lab_size = 3,
             show.diag = F)

```
```{r multilevel}
# step 1: Intercept only model ----
m1 <- comments %>% 
  lmer(sexism_centroid ~ (1|submission_id) + (1|author), ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

# step 2: Comment level variables ----
m2 <- comments %>% 
  lmer(sexism_centroid ~ comment_age +
         (1|submission_id) + (1|author), 
       data = ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

# step 3: Author and submission level variables ----
m3 <- comments %>% 
  lmer(sexism_centroid ~ comment_age +
         politics_comments + user_comments +
         thread_age +
         vote_average + vote_count + sexism_keywords + 
         MeToo +  
         (1|submission_id) + (1|author), 
       data = ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

# paste(genres, collapse = " + ")
m3b <- comments %>% 
  lmer(sexism_centroid ~ years + 
         politics_comments + vote_average + vote_count +
         sexism_keywords + racism_keywords +
         MeToo +
         `Science Fiction` + Action + Adventure + Fantasy + Crime + Thriller + Drama + Comedy + 
         Horror + Mystery + War + Animation + Family + History + Western + Romance + Music + 
         `TV Movie` + Documentary +
         (1|submission_id) + (1|author), 
       data = ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

# step 4: Random slope ----
m4 <- comments %>% 
  lmer(sexism_centroid ~ years + 
         politics_comments + vote_average + vote_count + sexism_keywords + 
         MeToo + 
         (1|submission_id) + (1+years|author), 
       data = ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

# step 5: cross-level interactions ----
m5a <- comments %>% 
  lmer(sexism_centroid ~ years * MeToo +
         politics_comments + vote_average + vote_count + sexism_keywords + 
         (1|submission_id) + (1+years|author), 
       data = ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

m5b <- comments %>% 
  lmer(sexism_centroid ~ years * sexism_keywords +
         politics_comments + vote_average + vote_count + MeToo + 
         (1|submission_id) + (1+years|author), 
       data = ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

m5c <- comments %>% 
  lmer(sexism_centroid ~ years * politics_comments +
         vote_average + vote_count + sexism_keywords + 
         MeToo +
         (1|submission_id) + (1+years|author), 
       data = ., REML = FALSE,
       control = lmerControl(optimizer ="Nelder_Mead")
       )

# Output ----
stargazer(m1, m2, m3c, type = "text")
get_models(m1, m2, m3, m4, out = "latex")

get_models(m2, m3, m3b, m3c)

anova(m1, m2, m3, m4)

summary(m1)
get_fixed(m1)
get_random(m1)
get_sum(m1)
# viz ----
# should use ranef() instead to get condVar, but its extremely slow

submissions <- comments %>% 
  arrange(Date) %>% 
  group_by(submission_id) %>% 
  slice_head(n=1) %>% 
  ungroup() %>% 
  select(!all_of(c("comment_id", "author", "score", "politics_comments"))) %>% 
  select(!ends_with("centroid")) %>% 
  select(!ends_with("_cmd"))

pred_values <- submissions %>% 
  left_join(predict_submission(m1)) %>% 
  left_join(predict_submission(m2)) %>% 
  left_join(predict_submission(m3c))

pred_values %>% 
  reshape2::melt(
    measure.vars = c("m1", "m2", "m2e", "m3c"),
    variable.name = "Model",
    value.name = "Ranef"
    ) %>% 
  as_tibble() %>% 
  ggplot(aes(Date, Ranef, color = Model, fill = Model)) +
  geom_smooth(method="loess", se=F)

pred_values %>% 
  # mutate(label = if_else(fit3c > 1 | fit3c < -.45, film_title, NA_character_)) %>% 
  ggplot(aes(Date, fit3c, label=label)) +
  # geom_point(alpha=.1) +
  geom_smooth(method = "lm", se=F, color="red") +
  # geom_smooth(method = "lm", formula = "y ~ poly(x, 2)", se=F, color="yellow") +
  # geom_smooth(method = "lm", formula = "y ~ poly(x, 5)", se=F, color="purple") +
  # geom_text(na.rm = T)

pred_values %>% 
  mutate(x = sexism_centroid - fit3b) %>% 
  arrange(x)

pred_values %>% 
  mutate(label = case_when(
    abs(Date - date_metoo) == min(abs(Date - date_metoo)) ~ "Alyssa Milano\nTweet",
    T ~ NA_character_)) %>% 
  group_by(label) %>% 
  mutate(label = if_else(!duplicated(label), label, NA_character_)) %>% 
  ungroup() %>% 
  ggplot(aes(Date, fit, label=label)) +
  # geom_line(aes(y=value, color = "Actual Means")) +
  # geom_smooth(aes(y=value, color = "Loess Smoothed Actual Means"), method = "loess",se=F) +
  geom_point(aes(color = "Predicted Means")) +
  # geom_smooth(aes(color = "Loess Smoothed Predicted Means"), method = "loess", se=F) +
  geom_vline(xintercept = as.numeric(date_metoo), linetype=2) +
  # scale_y_continuous(limits = c(0.025, 0.025)) +
  scale_x_date(date_breaks = "2 months", date_labels = "%b '%y",
               expand = expansion(mult = c(0, 0))) +
  scale_color_manual(values = lm_colors) +
  # ggrepel::geom_text_repel(color="black",
  #                          na.rm=T, show.legend = F,
  #                          # direction = "y", 
  #                          nudge_y = .05,
  #                          min.segment.length = Inf
  # ) +
  annotate("text", date_metoo, 0.075, label="Alyssa Milano Tweet", angle=90, vjust=-.3) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "top",
        legend.title = element_blank()) +
  labs(title = "Mean Predicted Values of Sexism-Related Comments Grouped by Submission",
       # subtitle = "Formula: CMD from sexism ~ log( #MeToo tweets) * cumulative frequency of #MeToo tweets",
       x = element_blank(),
       y = "CMD from Sexism Centroid")
```

```{r graveyard} 
# functions ----

lm_fitted <- function(.data, formula) {
  if (dplyr::is_grouped_df(.data)) {
      return(dplyr::do(.data, lm_fitted(., formula)))
    }
  model <- lm(formula=formula, data=.data)
  pred_vals = dplyr::tibble(
    fit = predict(model),
    se = predict(model, se.fit=T)$se.fit
  )
  .data <- bind_cols(.data, pred_vals)
  .data
}

get_random <- function(model) {
  VarCorr(model) %>% 
    as_tibble() %>%
    mutate(icc = vcov/sum(vcov),
           icc = if_else(grp=="Residual", NaN, icc)) %>% 
    select(!c(var2, sdcor))
}

# get_fixed <- function(model) {
#   summary(model)$coefficients %>% 
#     as_tibble(rownames = "Variable") %>% 
#     rename("Coef." = Estimate,
#            "S.E." = `Std. Error`,
#            "T" = `t value`)
# }

get_sum <- function(model) {
  fm <- get_fixed(model)
  rm <- get_random(model)
  ll <- logLik(model)[1]
  return(list(fm, rm, ll))
} 
# unique user ids ----

# There are many usernames "None", which are from deleted accounts. This makes them all unique
# If doing this, need to do something about politics_comments
comments <- comments %>% 
  mutate(
    author = if_else(
      author %in% c("[deleted]", "None"), 
      str_c("#", as.character(row_number())), # Hashtags are illegal characters in usernames, thus now these names are assured to be unique.
      author
    ),
    # Replacing politics_comments with the mean of the other rows
    politics_comments = if_else(str_detect(author, "#"), NaN, politics_comments),
    politics_comments = if_else(is.na(politics_comments), mean(politics_comments, na.rm=T), politics_comments)
  )

# Explore data ----
comments %>%
  select(date, submission_id) %>% 
  group_by(submission_id) %>% 
  summarise(days_since_sub = median(date-min(date))) %>% 
  filter(days_since_sub < 30) %>% 
  ggplot(aes(days_since_sub)) +
  stat_ecdf() +
  geom_vline(xintercept = 7, linetype=2) +
  scale_x_continuous(breaks = seq(0, 30, 1), limits = c(0,30), expand = c(0,0)) +
  labs(
    title = "The median number of days after a submission for comments to be posted ",
    x = "Number of days since submission",
    y = "Cumulative % of submissions"
  )

# Linear models ----
lin <- comments %>% 
  filter(Date >= ymd("2014-01-01"),
         Date < ymd("2022-01-01")) %>%
  lm(sexism_centroid ~ MeToo_before * MeToo, data = .)

summary(lin)

lin %>% 
  sjPlot::plot_model(type = "int", mdrt.values = "meansd")

lin %>% 
  broom::augment() %>% 
  arrange(.fitted) %>% 
  head()

lm_colors <- c(
  "Actual Means" = "green",
  "Loess Smoothed Actual Means" = "red",
  "Predicted Means" = "black",
  "Loess Smoothed Predicted Means" = "blue")

comments %>% 
  filter(Date >= ymd("2014-01-01"),
         Date < ymd("2022-01-01")) %>% 
  lm_fitted(sexism_centroid ~ MeToo_before * MeToo) %>%
  group_by(floor_date(Date, unit = "month")) %>% 
  summarise(fit = mean(fit),
            value = mean(sexism_centroid),
            Date = min(Date)) %>% 
  mutate(label = case_when(
    abs(Date - date_metoo) == min(abs(Date - date_metoo)) ~ "Alyssa Milano\nTweet",
    T ~ NA_character_)) %>% 
  group_by(label) %>% 
  mutate(label = if_else(!duplicated(label), label, NA_character_)) %>% 
  ungroup() %>% 
  ggplot(aes(Date, fit, label=label)) +
  # geom_line(aes(y=value, color = "Actual Means")) +
  geom_smooth(aes(y=value, color = "Loess Smoothed Actual Means"), method = "loess",se=F) +
  geom_line(aes(color = "Predicted Means")) +
  geom_smooth(aes(color = "Loess Smoothed Predicted Means"), method = "loess", se=F) +
  # scale_y_continuous(limits = c(-0.05, 0.05)) +
  scale_x_date(date_breaks = "2 months", date_labels = "%b '%y",
               expand = expansion(mult = c(0, 0))) +
  scale_color_manual(values = lm_colors) +
  ggrepel::geom_text_repel(color="black",
                           na.rm=T, show.legend = F,
                         # direction = "y", 
                         nudge_y = .02,
                         min.segment.length = Inf
           ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "top",
        legend.title = element_blank()) +
  labs(title = "Predicted Values of Sexism-Related Comments Over Time",
       subtitle = "Formula: CMD from sexism ~ log( #MeToo tweets) * cumulative frequency of #MeToo tweets",
       x = element_blank(),
       y = "Predicted Value")

comments %>% 
  group_by(submission_id) %>% 
  summarise(sexism_centroid = mean(sexism_centroid),
            sexism_keywords = min(sexism_keywords)) %>% 
  group_by(sexism_keywords) %>% 
  summarise(sexism_centroid = mean(sexism_centroid)) %>% 
  ggplot(aes(sexism_keywords, sexism_centroid)) +
  geom_bar(stat = "identity")

# Grouped by date ----
daily <- comments %>% 
  group_by(date = floor_date(date, "day")) %>%
  summarise(date = min(date),
            across(all_of(centroid_words), list(mean = mean, se = se))) %>% 
  pivot_longer(
    !date,
    names_to = c("keyword", ".value"),
    names_sep = "_"
  ) %>% 
  mutate(keyword = case_when(keyword=="blm" ~ "BLM",
                             keyword=="metoo" ~ "MeToo",
                             T ~ str_to_title(keyword))) %>% 
  left_join(twitter, by=c("date" = "Date", "keyword" = "movement"))

daily %>% 
  filter(date > ymd("2014-01-01")) %>% 
  arrange(desc(date)) %>% 
  head(20)
  
daily %>% 
  filter(date >= ymd("2014-01-01"),
         date < ymd("2022-01-01"),
         keyword %in% c("BLM", "MeToo")
         ) %>% 
  ggplot(aes(date, mean, color=keyword, fill=keyword, ymin=mean-se, ymax=mean+se)) + 
  #geom_line() +
  #geom_ribbon(alpha=.2) +
  geom_smooth(method="loess", se=T) +
  geom_vline(xintercept = as.numeric(ymd("2017-10-1")), linetype=2) +
  geom_vline(xintercept = as.numeric(ymd("2020-5-1")), linetype=2) +
  scale_x_date(date_breaks = "2 months", date_labels = "%m - %y",
               expand = expansion(mult = c(0, 0))) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

daily %>% 
  filter(date >= ymd("2014-01-01"),
         date < ymd("2022-01-01"),
         keyword %in% c("BLM", "MeToo")
  ) %>% 
  group_by(keyword) %>%
  mutate(fit = lm(mean ~ log10(tweets))$fitted.values) %>%
  slice_max(fit) %>% 
  ungroup() %>% 
  mutate(label = keyword) %>% 
  right_join(daily) %>% 
  filter(date >= ymd("2014-01-01"),
         date < ymd("2022-01-01"),
         keyword %in% c("BLM", "MeToo")
  ) %>% 
  ggplot(aes(tweets, mean, fill=keyword)) +
  geom_smooth(aes(color=keyword), method="lm") +
  scale_x_log10(
                limits = c(100,10^7),
                breaks = scales::log_breaks(6),
                labels = scales::comma,
                expand = expansion(mult = c(0, 0.02))
                ) +
  scale_y_continuous(
                     expand = expansion(mult = c(0, 0))) +
  labs(
    x = "Daily Count of Tweets Using the Keyword",
    y = "Mean Concept Mover's Distance From Keyword",
    color = "Keyword:",
    fill = "Keyword:",
    title = "Effect of Social Movements' Daily Visibility on Twitter on Reference to Those Movemnts in Film Discussion Threads on Reddit",
    caption = "Data from Jan. 1, 2014 to Dec. 31, 2021."
  ) +
  ggrepel::geom_text_repel(aes(y=fit, label=label), na.rm=T, show.legend = F,
                           direction = "y", nudge_y = .001,
                           min.segment.length = Inf
             ) +
  theme(legend.position = "none")

# At the comment level ----
bravo <- comments %>% 
  filter(date >= ymd("2014-01-01"),
         date < ymd("2022-01-01")) %>% 
  reshape2::melt(measure.vars=centroid_words,
       variable.name="keyword",
       value.name = "CMD")

bravo %>%
  ggplot(aes(date, CMD, color=keyword, fill=keyword)) + 
  geom_smooth(method = lm) +
  geom_vline(xintercept = as.numeric(ymd("2017-10-1")), linetype=2) +
  geom_vline(xintercept = as.numeric(ymd("2020-5-1")), linetype=2) +
  scale_x_date(date_breaks = "2 months", date_labels = "%m - %y",
               expand = expansion(mult = c(0, 0))) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

# Table export ----

c(m1,m2,m3n) %>% 
  map(get_random) %>% 
  bind_rows(.id = "Model") %>% 
  rename(vpc = icc) %>%
  mutate(zzz = NA) %>% 
  pivot_wider(
    names_from = "Model",
    values_from = 4:6,
    names_glue = "M{Model}_{.value}"
  ) %>% 
  select(order(colnames(.))) %>%
  select(grp, var1, everything()) %>%
  mutate(across(where(is.numeric), ~ round(.x, digits = 2))) %>% 
      # rename_with(~ str_c("{", .x, "}")) %>% 
      xtable() %>% 
      print(
        include.rownames = F,
        include.colnames = F,
        #math.style.negative = T,
        print.results = F,
        booktabs = T,
        comment = F,
        only.contents = T
      ) %>% 
  str_replace_all("\\\\", "\\\\\\\\") %>% 
  print()

ll <- logLik(model)[1]

map(c(m1,m2),
  ~ logLik(.)[1]) %>% 
  as_tibble_col() %>% 
  unnest_longer(value) %>% 
  mutate("name" = 1:nrow(.),
         # value = str_c("{", as.character(as.integer(value)), "}")
         ) %>% 
  group_by(name) %>% 
  mutate(
         y = NA,
         z = NA) %>% 
  ungroup() %>% 
  pivot_wider(names_from = "name",
              values_from = c(value, y, z),
              names_glue = "M{name}_{.value}") %>% 
  mutate(a = "LogLik",
         b = NA) %>% 
  select(order(colnames(.))) %>% 
  xtable() %>% 
      print(
        include.rownames = F,
        include.colnames = F,
        #math.style.negative = T,
        print.results = F,
        booktabs = T,
        comment = F,
        only.contents = T
      ) %>%
  # str_replace_all("[-\d\.]+")
  str_replace_all("\\\\", "\\\\\\\\") %>% 
  cat()
```