{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "controversial-victorian",
   "metadata": {},
   "source": [
    "# Collecting Reddit comments\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welcome-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "coral-tension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'D:\\\\Python\\\\Thesis\\\\utils.py'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-referral",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "complicated-billy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define search parameters\n",
    "comment_attributes = ('comment.id',\n",
    "                      'submission.id',\n",
    "                      'comment.body',\n",
    "                      'str(comment.author)', # need to convert author to string, otherwise it's a class instance.\n",
    "                      'comment.score',\n",
    "                      'comment.created_utc')\n",
    "\n",
    "# Define comment_limit, the number of comments to be obtained\n",
    "comment_limit = 1200000\n",
    "\n",
    "# Define location and name of SQL database, create a connection object\n",
    "sql_db = './data/film_discussions'\n",
    "conn = sqlite3.connect(sql_db)\n",
    "\n",
    "# Creating a Reddit-instance in PRAW with my personal Reddit username, password etc.\n",
    "# Before handing in the project I removed the praw.ini file from this folder, which is why it now gives an error message\n",
    "# See: https://praw.readthedocs.io/en/latest/getting_started/configuration/prawini.html\n",
    "reddit_praw_id = \"Jarik\"\n",
    "reddit = praw.Reddit(reddit_praw_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-dance",
   "metadata": {},
   "source": [
    "## Create SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electric-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_create_comments_table = \"\"\"CREATE TABLE IF NOT EXISTS comments (\n",
    "                                comment_id text PRIMARY KEY,\n",
    "                                submission_id text,\n",
    "                                body text,\n",
    "                                author text,\n",
    "                                score integer,\n",
    "                                created integer,\n",
    "                                FOREIGN KEY (submission_id) REFERENCES submissions (submission_id)\n",
    "                            );\"\"\"\n",
    "\n",
    "with conn:\n",
    "    utils.interact_with_db(conn, sql_create_comments_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-violence",
   "metadata": {},
   "source": [
    "## Determine for which submissions I still need to collect comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cognitive-river",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 481 submissions for which no comments have been gathered\n"
     ]
    }
   ],
   "source": [
    "# List submission IDs\n",
    "# Because I didn't download all the comments in one go, the difference_update allows me to continue where I left off.  \n",
    "\n",
    "with conn:\n",
    "    submission_ids = utils.get_submission_ids(conn, 'submissions')\n",
    "    sub_ids_comments = utils.get_submission_ids(conn, 'comments')\n",
    "\n",
    "# Update difference   \n",
    "submission_ids.difference_update(sub_ids_comments)\n",
    "print(f\"There are {len(submission_ids)} submissions for which no comments have been gathered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-diagnosis",
   "metadata": {},
   "source": [
    "## Collect comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "domestic-genre",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                      | 0/1200000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2022-02-15 23:58:47.740175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████▊                                                              | 86615/1200000 [19:12:58<247:00:54,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86615\n",
      "{'bgnl7y', '7jwxnd', '1v2iqp', 'nx3jdq'}\n",
      "Finished at 2022-02-16 19:11:46.681704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting at {datetime.now()}\")\n",
    "comment_count = 0\n",
    "n_comment_attributes = len(comment_attributes)\n",
    "submission_errors = set()\n",
    "\n",
    "with tqdm(total=comment_limit) as pbar:\n",
    "    for submission_id in submission_ids:\n",
    "\n",
    "        if comment_count < comment_limit:\n",
    "            try:\n",
    "                # Collect comments\n",
    "                comments_data = utils.get_comments_data(reddit, submission_id, comment_attributes=comment_attributes)\n",
    "\n",
    "                # Save the comments to the database\n",
    "                with conn:\n",
    "                    n_comments = utils.add_rows(conn, 'comments', n_comment_attributes, comments_data)\n",
    "\n",
    "                # Update counter and tqdm\n",
    "                comment_count += n_comments\n",
    "                pbar.update(n_comments)\n",
    "\n",
    "            except:\n",
    "                # Sometimes my internet cuts off for a few minutes, or some other error happens.\n",
    "                # This try/except statement allows for the loop to continue.\n",
    "                # print(f\"Something went wrong at {datetime.now()} with submission {submission_id}\")\n",
    "                submission_errors.add(submission_id)\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Reached comment limit at {datetime.now()} with submission {submission_id}\")\n",
    "            print(f\"Collected {comment_count} comments\")\n",
    "            break\n",
    "        \n",
    "with open('raised_errors.txt','a') as f:\n",
    "    for i in submission_errors:\n",
    "        f.write(i)\n",
    "        f.write('\\n')        \n",
    "        \n",
    "print(comment_count)\n",
    "print(submission_errors)\n",
    "print(f\"Finished at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-witch",
   "metadata": {},
   "source": [
    "## Submissions that raised errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ruled-summer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "with open('raised_errors.txt','r') as f:\n",
    "    errors = f.read()\n",
    "    \n",
    "errors = set(errors.splitlines())\n",
    "\n",
    "# Manual inspection found that these threads had been removed or contained no comments\n",
    "removed = {'ft2qny', 'cu3hzi', 'gkfeb1', 'ncnw25', 'c7xnp5', 'qd6y5i', 'ri7gfm'}\n",
    "\n",
    "errors.difference_update(removed)\n",
    "\n",
    "# That raised errors a second time\n",
    "errors = {'bgnl7y', '7jwxnd', '1v2iqp', 'nx3jdq'}\n",
    "\n",
    "print(len(errors))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "express-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ids = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "varied-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2295439,)\n"
     ]
    }
   ],
   "source": [
    "print(utils.interact_with_db(conn, \"SELECT COUNT(*) FROM comments\", \"cur.fetchone()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "billion-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2409188,)\n"
     ]
    }
   ],
   "source": [
    "print(utils.interact_with_db(conn, \"SELECT COUNT(*) FROM comments\", \"cur.fetchone()\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
