{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorporated-brown",
   "metadata": {},
   "source": [
    "# Collecting Reddit submissions\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import regex as re\n",
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "import utils\n",
    "from params import sql_db, discussionarchive_submissions_pushshift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-plate",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search parameters\n",
    "# the keys are the parameter names (see https://pushshift.io/api-parameters/ for possible parameters)\n",
    "param_dict = {'subreddit':'discussionarchive',\n",
    "              'size':1000, # 1000 is the maximum number that can be collected per single request. No reason to change this.\n",
    "              'is_self': \"false\",\n",
    "             }\n",
    "\n",
    "# Keys to collect from submissions\n",
    "submission_keys = ('id', 'title', 'score', 'num_comments', 'url', 'created_utc')\n",
    "\n",
    "# Define submission_limit, the number of submissions to be obtained by the API\n",
    "submission_limit = 100000\n",
    "\n",
    "# Define location and name of SQL database, create a connection object\n",
    "conn = sqlite3.connect(sql_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn:\n",
    "    submission_ids = utils.get_submission_ids(conn, 'submissions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-explanation",
   "metadata": {},
   "source": [
    "## Collect submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://github.com/SeyiAgboola/Reddit-Data-Mining/blob/master/Using_Pushshift_Module_to_extract_Submissions.ipynb\n",
    "print(f\"Starting at {datetime.now()}\")\n",
    "sub_count = 0\n",
    "archive_data = set()\n",
    "\n",
    "# Collect first set of submissions\n",
    "# We need to run this function outside the loop first to get the updated before variable\n",
    "data = utils.get_pushshift_data(param_dict)\n",
    "\n",
    "print(f\"The youngest submission that fits the criteria is from: {datetime.fromtimestamp(data[0]['created_utc'])}\")\n",
    "\n",
    "while len(data) > 0: \n",
    "    if sub_count < submission_limit:\n",
    "        \n",
    "        for submission in data:\n",
    "            try:\n",
    "                url = re.findall(\"comments/([^/]+)\", submission['url'])[0]\n",
    "                archive_data.add(url)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Set the new 'before' parameter\n",
    "        param_dict['before'] = data[-1]['created_utc']\n",
    "\n",
    "        # Collect next set of submissions\n",
    "        data = utils.get_pushshift_data(param_dict)\n",
    "        \n",
    "        sub_count += 100\n",
    "\n",
    "    else:\n",
    "        print(f\"Reached submission limit at {datetime.now()}\")\n",
    "        print(f\"Didn't collect submissions posted before {datetime.fromtimestamp(param_dict['before'])}\")\n",
    "        break\n",
    "    \n",
    "\n",
    "print(f\"Finished at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(archive_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(discussionarchive_submissions_pushshift,'w') as f:\n",
    "    for i in archive_data:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(submission_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_data.difference_update(submission_ids)\n",
    "\n",
    "print(len(archive_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-extra",
   "metadata": {},
   "source": [
    "## Find submission metadata\n",
    "\n",
    "Only works for 13 submissions. Pushshift doesn't seem to be complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = \",\".join(archive_data)\n",
    "url= f\"https://api.pushshift.io/reddit/search/submission/?ids={ids}\"\n",
    "\n",
    "r = requests.get(url)\n",
    "data = r.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    print(i['title'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
