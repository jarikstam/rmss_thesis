{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rental-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geographic-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dataset = 'data/comments_analysis.csv'\n",
    "politics_comments_file = 'data/politics_comments.csv'\n",
    "\n",
    "# Define search parameters\n",
    "# the keys are the parameter names (see https://pushshift.io/api-parameters/ for possible parameters)\n",
    "param_dict = {'metadata':'true',\n",
    "              'subreddit':'politics',\n",
    "              'size':0\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "internal-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pushshift_data(param_dict, url='https://api.pushshift.io/reddit/search/submission/?'):\n",
    "    \"\"\"\n",
    "    Return data from the pushshift API\n",
    "    Based on: https://github.com/SeyiAgboola/Reddit-Data-Mining/blob/master/Using_Pushshift_Module_to_extract_Submissions.ipynb\n",
    "    :param param_dict: A dictionary with key+value pairs to feed to the API\n",
    "    :param url: The URL of the pushshift API\n",
    "    :return: A json object\n",
    "    \"\"\"\n",
    "    for k, v in param_dict.items():\n",
    "        url = f'{url}{k}={v}&'\n",
    "\n",
    "    url = url[:-1]\n",
    "    r = requests.get(url)\n",
    "    assert r.status_code == 200, r.status_code\n",
    "    data = r.json()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = set(pd.read_csv(analysis_dataset)[\"author\"].to_list())\n",
    "original_authors_length = len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "detected-democrat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291415\n",
      "5\n",
      "291410\n"
     ]
    }
   ],
   "source": [
    "collected_authors = set(pd.read_csv(politics_comments_file)[\"author\"].tolist())\n",
    "authors.difference_update(collected_authors)\n",
    "\n",
    "print(original_authors_length)\n",
    "print(len(authors))\n",
    "print(len(collected_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sweet-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(authors)+len(collected_authors) == original_authors_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "collected-hometown",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "n_comments_out = []\n",
    "authors_out = []\n",
    "author_errors = set()\n",
    "end_time = time.time() + 60*60*40\n",
    "\n",
    "for i, author in enumerate(tqdm(authors)):\n",
    "    param_dict['author'] = author\n",
    "    \n",
    "    data = None\n",
    "    time.sleep(0.5) # current rate limit is 120/min according to https://api.pushshift.io/meta\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            data = get_pushshift_data(param_dict, url=\"https://api.pushshift.io/reddit/search/comment/?\")\n",
    "        except AssertionError:\n",
    "            first_error = sys.exc_info()[1].args[0]\n",
    "            new_error = sys.exc_info()[1].args[0]\n",
    "\n",
    "            while new_error == first_error:\n",
    "                \n",
    "                if time.time() > end_time:\n",
    "                    time_to_stop = True\n",
    "                    print(f\"Finished at {datetime.now()} with user {authors_out[-1]} while the server was down\")\n",
    "                    break\n",
    "\n",
    "                time.sleep(60)\n",
    "                try:\n",
    "                    data = get_pushshift_data(param_dict, url=\"https://api.pushshift.io/reddit/search/comment/?\")\n",
    "                    new_error = None\n",
    "                except AssertionError:\n",
    "                    new_error = sys.exc_info()[1].args[0]\n",
    "            else:\n",
    "                time_to_stop = False\n",
    "            \n",
    "            if time_to_stop:\n",
    "                break\n",
    "\n",
    "        if data is not None:\n",
    "            authors_out.append(author)\n",
    "            n_comments_out.append(data['metadata']['total_results'])\n",
    "            \n",
    "        if (len(n_comments_out) == 2000) or (i == len(authors)-1):\n",
    "            df = pd.DataFrame({\n",
    "                'author' : authors_out,\n",
    "                'politics_comments' : n_comments_out\n",
    "            })\n",
    "\n",
    "            df.to_csv(politics_comments_file, mode='a', header=not os.path.exists(politics_comments_file), index=False)\n",
    "            n_comments_out = []\n",
    "            authors_out = []\n",
    "\n",
    "            if time.time() > end_time:\n",
    "                print(f\"Finished at {datetime.now()} with user {author}\")\n",
    "                break\n",
    "\n",
    "    except:\n",
    "        print(f\"Something went wrong at {datetime.now()} with user {author}\")\n",
    "        author_errors.add(author)\n",
    "        \n",
    "if len(n_comments_out) > 0:\n",
    "    df = pd.DataFrame({\n",
    "        'author' : authors_out,\n",
    "        'politics_comments' : n_comments_out\n",
    "    })\n",
    "\n",
    "    df.to_csv(politics_comments_file, mode='a', header=not os.path.exists(politics_comments_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "comparative-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(author_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "renewable-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pols = pd.read_csv(politics_comments_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lyric-specialist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>politics_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291410</th>\n",
       "      <td>niorec</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291411</th>\n",
       "      <td>Doom_Marine_II</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291412</th>\n",
       "      <td>Muouy</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291413</th>\n",
       "      <td>wullymammith</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291414</th>\n",
       "      <td>speakharp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  politics_comments\n",
       "291410          niorec                  6\n",
       "291411  Doom_Marine_II                  0\n",
       "291412           Muouy                 21\n",
       "291413    wullymammith                412\n",
       "291414       speakharp                  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pols.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hybrid-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "complicated-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "pols = pols.convert_dtypes()\n",
    "comments = comments.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sufficient-circle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments = comments.merge(pols, how=\"left\", on=\"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acute-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(analysis_dataset, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
